{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_29154/3545525767.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'bias' 'rumor'\n",
      " 'clickbait' 'hate' 'junksci']\n",
      "[1 0]\n",
      "type\n",
      "1    649783\n",
      "0    218564\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')\n",
    "\n",
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,  \"clickbait\": 1,   \"political\": 1,\n",
    "             \"reliable\": 0} #, \"unreliable\": 1\n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].unique())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].unique())\n",
    "print(df[\"type\"].value_counts())\n",
    "\n",
    "y = df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = df[\"content\"]\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (694677, 10000)\n",
      "X_val.shape: (86835, 10000)\n",
      "X_test.shape: (86835, 10000)\n",
      "\n",
      "y_train.shape: (694677,)\n",
      "y_val.shape: (86835,)\n",
      "y_test.shape: (86835,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_val.shape: {X_val.shape}\")\n",
    "print(f\"X_test.shape: {X_test.shape}\")\n",
    "print()\n",
    "\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"y_val.shape: {y_val.shape}\")\n",
    "print(f\"y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1412/1412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 91ms/step - accuracy: 0.9359 - auc_6: 0.9652 - f1_score: 0.8685 - loss: 0.1839 - precision_6: 0.9418 - recall_6: 0.9760 - val_accuracy: 0.9649 - val_auc_6: 0.9895 - val_f1_score: 0.8808 - val_loss: 0.0982 - val_precision_6: 0.9679 - val_recall_6: 0.9865\n",
      "Epoch 2/10\n",
      "\u001b[1m1412/1412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 92ms/step - accuracy: 0.9750 - auc_6: 0.9942 - f1_score: 0.8902 - loss: 0.0701 - precision_6: 0.9783 - recall_6: 0.9889 - val_accuracy: 0.9690 - val_auc_6: 0.9904 - val_f1_score: 0.8986 - val_loss: 0.0913 - val_precision_6: 0.9747 - val_recall_6: 0.9846\n",
      "Epoch 3/10\n",
      "\u001b[1m1412/1412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 92ms/step - accuracy: 0.9881 - auc_6: 0.9983 - f1_score: 0.9083 - loss: 0.0339 - precision_6: 0.9901 - recall_6: 0.9943 - val_accuracy: 0.9693 - val_auc_6: 0.9869 - val_f1_score: 0.9129 - val_loss: 0.1075 - val_precision_6: 0.9743 - val_recall_6: 0.9856\n",
      "Epoch 4/10\n",
      "\u001b[1m1412/1412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 91ms/step - accuracy: 0.9930 - auc_6: 0.9992 - f1_score: 0.9187 - loss: 0.0208 - precision_6: 0.9944 - recall_6: 0.9963 - val_accuracy: 0.9694 - val_auc_6: 0.9825 - val_f1_score: 0.9162 - val_loss: 0.1304 - val_precision_6: 0.9726 - val_recall_6: 0.9875\n",
      "Epoch 5/10\n",
      "\u001b[1m1412/1412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 92ms/step - accuracy: 0.9951 - auc_6: 0.9995 - f1_score: 0.9247 - loss: 0.0148 - precision_6: 0.9961 - recall_6: 0.9974 - val_accuracy: 0.9692 - val_auc_6: 0.9815 - val_f1_score: 0.9271 - val_loss: 0.1377 - val_precision_6: 0.9742 - val_recall_6: 0.9856\n"
     ]
    }
   ],
   "source": [
    "# Alternate model\n",
    "input_layer = Input(shape=(X_train.shape[1],), sparse=True)\n",
    "x1 = layers.Dense(1000, activation=\"relu\")(input_layer)\n",
    "x1 = layers.Dropout(0.4)(x1)\n",
    "x2 = layers.Dense(500, activation=\"relu\")(x1)\n",
    "x2 = layers.Dropout(0.2)(x2)\n",
    "# Skip connection\n",
    "x3 = layers.concatenate([x1, x2])\n",
    "x3 = layers.Dense(100, activation=\"relu\")(x3)\n",
    "x3 = layers.Dropout(0.2)(x3)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x3)\n",
    "\n",
    "NN = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "#Initialize the NN\n",
    "NN.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.F1Score()  # TensorFlow 2.6+ only\n",
    "    ]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "#Train the NN\n",
    "history = NN.fit(\n",
    "    X_train,y_train,\n",
    "    validation_data = (X_val,y_val),\n",
    "    epochs = 10,\n",
    "    batch_size = 512,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 139ms/step - accuracy: 0.9296 - auc_8: 0.9609 - f1_score: 0.8600 - loss: 0.1942 - precision_8: 0.9327 - recall_8: 0.9775 - val_accuracy: 0.9630 - val_auc_8: 0.9891 - val_f1_score: 0.8650 - val_loss: 0.1040 - val_precision_8: 0.9709 - val_recall_8: 0.9800\n",
      "Epoch 2/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 139ms/step - accuracy: 0.9749 - auc_8: 0.9944 - f1_score: 0.8804 - loss: 0.0708 - precision_8: 0.9784 - recall_8: 0.9882 - val_accuracy: 0.9671 - val_auc_8: 0.9893 - val_f1_score: 0.8918 - val_loss: 0.0976 - val_precision_8: 0.9716 - val_recall_8: 0.9849\n",
      "Epoch 3/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 141ms/step - accuracy: 0.9880 - auc_8: 0.9982 - f1_score: 0.8987 - loss: 0.0347 - precision_8: 0.9900 - recall_8: 0.9940 - val_accuracy: 0.9677 - val_auc_8: 0.9870 - val_f1_score: 0.9040 - val_loss: 0.1105 - val_precision_8: 0.9734 - val_recall_8: 0.9839\n",
      "Epoch 4/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 140ms/step - accuracy: 0.9930 - auc_8: 0.9993 - f1_score: 0.9114 - loss: 0.0210 - precision_8: 0.9945 - recall_8: 0.9961 - val_accuracy: 0.9678 - val_auc_8: 0.9829 - val_f1_score: 0.9139 - val_loss: 0.1285 - val_precision_8: 0.9713 - val_recall_8: 0.9863\n",
      "Epoch 5/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 141ms/step - accuracy: 0.9954 - auc_8: 0.9995 - f1_score: 0.9180 - loss: 0.0142 - precision_8: 0.9962 - recall_8: 0.9976 - val_accuracy: 0.9684 - val_auc_8: 0.9811 - val_f1_score: 0.9175 - val_loss: 0.1440 - val_precision_8: 0.9738 - val_recall_8: 0.9844\n",
      "Epoch 6/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 140ms/step - accuracy: 0.9961 - auc_8: 0.9995 - f1_score: 0.9208 - loss: 0.0122 - precision_8: 0.9969 - recall_8: 0.9979 - val_accuracy: 0.9687 - val_auc_8: 0.9800 - val_f1_score: 0.9214 - val_loss: 0.1496 - val_precision_8: 0.9740 - val_recall_8: 0.9846\n",
      "Epoch 7/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 140ms/step - accuracy: 0.9968 - auc_8: 0.9997 - f1_score: 0.9245 - loss: 0.0099 - precision_8: 0.9975 - recall_8: 0.9982 - val_accuracy: 0.9683 - val_auc_8: 0.9776 - val_f1_score: 0.9214 - val_loss: 0.1587 - val_precision_8: 0.9698 - val_recall_8: 0.9885\n"
     ]
    }
   ],
   "source": [
    "# Standard model\n",
    "# Standard model with corrected tensor flow\n",
    "input_layer = Input(shape=(X_train.shape[1],), sparse=True)\n",
    "x1 = layers.Dense(1000, activation=\"relu\")(input_layer)\n",
    "x1 = layers.Dropout(0.4)(x1)\n",
    "x2 = layers.Dense(500, activation=\"relu\")(x1)\n",
    "x2 = layers.Dropout(0.2)(x2)\n",
    "x3 = layers.Dense(100, activation=\"relu\")(x2)\n",
    "x3 = layers.Dropout(0.2)(x3)\n",
    "x4 = layers.Dense(25, activation=\"relu\")(x3)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x4)\n",
    "\n",
    "NN = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "\n",
    "#Initialize the NN\n",
    "NN.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.F1Score()  # TensorFlow 2.6+ only\n",
    "    ]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "#Train the NN\n",
    "history = NN.fit(\n",
    "    X_train,y_train,\n",
    "    validation_data = (X_val,y_val),\n",
    "    epochs = 10,\n",
    "    batch_size = 1024,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "output_path = \"models/nn_classifier9_f1_val_0.9214.keras\"\n",
    "NN.save(output_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve analysis\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learinig curves analysis\n",
    "# Plot training & validation accuracy/loss values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models \n",
    "## nn_classifier1_f1_val_0.9328.keras - runtime: 16,6 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 256 bach size\n",
    "-   val_accuracy: 0.9695 - val_auc: 0.9820 - val_f1_score: 0.9328 - val_loss: 0.1359 - val_precision: 0.9732 - val_recall: 0.9870\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0917\n",
    "* Test Accuracy: 0.9685\n",
    "* Test Precision: 0.9696\n",
    "* Test Recall: 0.9895\n",
    "* Test AUC: 0.9901\n",
    "* Test F1 Score: 0.8937\n",
    "\n",
    "## nn_classifier2_f1_val_0.9304.keras - runtime: 10,75 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9690 - val_auc_2: 0.9802 - val_f1_score: 0.9304 - val_loss: 0.1467 - val_precision_2: 0.9744 - val_recall_2: 0.9851\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0903\n",
    "* Test Accuracy: 0.9685\n",
    "* Test Precision: 0.9751\n",
    "* Test Recall: 0.9836\n",
    "* Test AUC: 0.9908\n",
    "* Test F1 Score: 0.8951\n",
    "\n",
    "## nn_classifier3_f1_val_0.9228.keras - runtime: 10,5 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9703 - val_auc_4: 0.9828 - val_f1_score: 0.9228 - val_loss: 0.1242 - val_precision_4: 0.9752 - val_recall_4: 0.9860\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0919\n",
    "* Test Accuracy: 0.9689\n",
    "* Test Precision: 0.9729\n",
    "* Test Recall: 0.9866\n",
    "* Test AUC: 0.9897\n",
    "* Test F1 Score: 0.8829\n",
    "\n",
    "## nn_classifier4_f1_val_0.9292\n",
    "-   5000 (ReLu)(0.4 dropoff),1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9697 - val_auc_5: 0.9818 - val_f1_score: 0.9292 - val_loss: 0.1331 - val_precision_5: 0.9750 - val_recall_5: 0.9854\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0919\n",
    "* Test Accuracy: 0.9689\n",
    "* Test Precision: 0.9729\n",
    "* Test Recall: 0.9866\n",
    "* Test AUC: 0.9897\n",
    "* Test F1 Score: 0.8829\n",
    "\n",
    "## nn_classifier5_f1_val_0.9292 - runtime 10,75\n",
    "-   Alternate model\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9692 - val_auc_6: 0.9815 - val_f1_score: 0.9271 - val_loss: 0.1377 - val_precision_6: 0.9742 - val_recall_6: 0.9856\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0919\n",
    "* Test Accuracy: 0.9684\n",
    "* Test Precision: 0.9737\n",
    "* Test Recall: 0.9849\n",
    "* Test AUC: 0.9901\n",
    "* Test F1 Score: 0.8983\n",
    "\n",
    "## nn_classifier6_f1_val_0.916 - 16 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 256 bach size\n",
    "\n",
    "## models/nn_classifier7_f1_val_0.9266.keras - 10 min - NO unreliable\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0928\n",
    "* Test Accuracy: 0.9678\n",
    "* Test Precision: 0.9744\n",
    "* Test Recall: 0.9827\n",
    "* Test AUC: 0.9901\n",
    "* Test F1 Score: 0.8861\n",
    "\n",
    "## nn_classifier8_f1_val_0.6775 - 5 min - ONLY reliable and fake\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   7 epochs, 1024 bach size\n",
    "\n",
    "### Model evaluation key data\n",
    "* Test Loss: 17.5059\n",
    "* Test Accuracy: 0.3997\n",
    "* Test Precision: 0.7829\n",
    "* Test Recall: 0.2741\n",
    "* Test AUC: 0.5415\n",
    "* Test F1 Score: 0.6991\n",
    "\n",
    "## nn_classifier9_f1_val_0.9214 - 11 min - NO unreliable\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   7 epochs, 1024 bach size\n",
    "\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0991\n",
    "* Test Accuracy: 0.9665\n",
    "* Test Precision: 0.9716\n",
    "* Test Recall: 0.9840\n",
    "* Test AUC: 0.9896\n",
    "* Test F1 Score: 0.8901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "input_path = \"models/nn_classifier1_f1_val_0.9328.keras\"\n",
    "NN = tf.keras.models.load_model(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2714/2714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.9665 - auc_8: 0.9891 - f1_score: 0.8912 - loss: 0.0990 - precision_8: 0.9718 - recall_8: 0.9839\n",
      "* Test Loss: 0.0991\n",
      "* Test Accuracy: 0.9665\n",
      "* Test Precision: 0.9716\n",
      "* Test Recall: 0.9840\n",
      "* Test AUC: 0.9896\n",
      "* Test F1 Score: 0.8901\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_results = NN.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"* Test Loss: {test_results[0]:.4f}\")\n",
    "print(f\"* Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"* Test Precision: {test_results[2]:.4f}\")\n",
    "print(f\"* Test Recall: {test_results[3]:.4f}\")\n",
    "print(f\"* Test AUC: {test_results[4]:.4f}\")\n",
    "print(f\"* Test F1 Score: {test_results[5]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
