{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_61999/735551807.py:2: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "reliable      218564\n",
      "political     194518\n",
      "bias          133232\n",
      "fake          104883\n",
      "conspiracy     97314\n",
      "rumor          56445\n",
      "clickbait      27412\n",
      "junksci        14040\n",
      "satire         13160\n",
      "hate            8779\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "1    649783\n",
      "0    218564\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/995,000_row_cleaned.csv')\n",
    "\n",
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,  \"clickbait\": 1,   \"political\": 1,\n",
    "             \"reliable\": 0} # unreliable isn't kept because it's not conclusive data \n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].value_counts())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].value_counts())\n",
    "\n",
    "# Split data \n",
    "texts = df[\"content\"]\n",
    "y = df[\"type\"]\n",
    "\n",
    "# Create a instances of the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # make sure the number of features is 10000\n",
    ")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, y, \n",
    "    test_size=0.2,\n",
    "    shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5,\n",
    "    shuffle=False)\n",
    "\n",
    "# Fit and transform the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)  \n",
    "X_val = vectorizer.transform(X_val)       \n",
    "X_test = vectorizer.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (694677, 10000) \n",
      "y_train.shape: (694677,) \n",
      "\n",
      "X_val.shape: (86835, 10000) \n",
      "y_val.shape: (86835,)\n",
      "\n",
      "X_test.shape: (86835, 10000) \n",
      "y_test.shape: (86835,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the shape of the data\n",
    "print(f\"X_train.shape: {X_train.shape} \\ny_train.shape: {y_train.shape} \\n\")\n",
    "print(f\"X_val.shape: {X_val.shape} \\ny_val.shape: {y_val.shape}\\n\")\n",
    "print(f\"X_test.shape: {X_test.shape} \\ny_test.shape: {y_test.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 141ms/step - f1_score: 0.8583 - loss: 0.2301 - precision_1: 0.9679 - recall_1: 0.8892 - weighted_f1_score: 0.6664 - val_f1_score: 0.8477 - val_loss: 0.1406 - val_precision_1: 0.9779 - val_recall_1: 0.9498 - val_weighted_f1_score: 0.8477\n",
      "Epoch 2/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 141ms/step - f1_score: 0.8645 - loss: 0.0973 - precision_1: 0.9867 - recall_1: 0.9682 - weighted_f1_score: 0.6777 - val_f1_score: 0.8572 - val_loss: 0.1218 - val_precision_1: 0.9810 - val_recall_1: 0.9559 - val_weighted_f1_score: 0.8572\n",
      "Epoch 3/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 143ms/step - f1_score: 0.8771 - loss: 0.0410 - precision_1: 0.9953 - recall_1: 0.9864 - weighted_f1_score: 0.7018 - val_f1_score: 0.8575 - val_loss: 0.1163 - val_precision_1: 0.9755 - val_recall_1: 0.9706 - val_weighted_f1_score: 0.8575\n",
      "Epoch 4/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 141ms/step - f1_score: 0.8894 - loss: 0.0221 - precision_1: 0.9974 - recall_1: 0.9926 - weighted_f1_score: 0.7261 - val_f1_score: 0.8864 - val_loss: 0.1420 - val_precision_1: 0.9738 - val_recall_1: 0.9747 - val_weighted_f1_score: 0.8864\n",
      "Epoch 5/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 141ms/step - f1_score: 0.9021 - loss: 0.0145 - precision_1: 0.9983 - recall_1: 0.9951 - weighted_f1_score: 0.7524 - val_f1_score: 0.9022 - val_loss: 0.1646 - val_precision_1: 0.9752 - val_recall_1: 0.9734 - val_weighted_f1_score: 0.9022\n",
      "Epoch 6/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 140ms/step - f1_score: 0.9089 - loss: 0.0113 - precision_1: 0.9987 - recall_1: 0.9963 - weighted_f1_score: 0.7669 - val_f1_score: 0.8984 - val_loss: 0.1693 - val_precision_1: 0.9695 - val_recall_1: 0.9824 - val_weighted_f1_score: 0.8984\n",
      "Epoch 7/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 141ms/step - f1_score: 0.9145 - loss: 0.0099 - precision_1: 0.9988 - recall_1: 0.9969 - weighted_f1_score: 0.7790 - val_f1_score: 0.9132 - val_loss: 0.1718 - val_precision_1: 0.9754 - val_recall_1: 0.9726 - val_weighted_f1_score: 0.9132\n",
      "Epoch 8/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 141ms/step - f1_score: 0.9242 - loss: 0.0084 - precision_1: 0.9990 - recall_1: 0.9975 - weighted_f1_score: 0.8008 - val_f1_score: 0.9131 - val_loss: 0.1649 - val_precision_1: 0.9737 - val_recall_1: 0.9768 - val_weighted_f1_score: 0.9131\n",
      "Epoch 9/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 141ms/step - f1_score: 0.9264 - loss: 0.0078 - precision_1: 0.9991 - recall_1: 0.9977 - weighted_f1_score: 0.8059 - val_f1_score: 0.9184 - val_loss: 0.1752 - val_precision_1: 0.9772 - val_recall_1: 0.9700 - val_weighted_f1_score: 0.9184\n",
      "Epoch 10/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 140ms/step - f1_score: 0.9266 - loss: 0.0074 - precision_1: 0.9991 - recall_1: 0.9978 - weighted_f1_score: 0.8062 - val_f1_score: 0.9181 - val_loss: 0.1746 - val_precision_1: 0.9730 - val_recall_1: 0.9774 - val_weighted_f1_score: 0.9181\n"
     ]
    }
   ],
   "source": [
    "# Standard model\n",
    "# Define the model\n",
    "input_layer = Input(shape=(X_train.shape[1],), sparse=True)\n",
    "x1 = layers.Dense(1000, activation=\"relu\")(input_layer) # 1st hidden layer with 1000 neurons\n",
    "x1 = layers.Dropout(0.4)(x1)                            # Dropout layer to prevent overfitting \n",
    "x2 = layers.Dense(500, activation=\"relu\")(x1)           # 2nd hidden layer with 500 neurons\n",
    "x2 = layers.Dropout(0.2)(x2)                            # Dropout layer to prevent overfitting\n",
    "x3 = layers.Dense(100, activation=\"relu\")(x2)           # 3rd hidden layer with 100 neurons\n",
    "x4 = layers.Dense(25, activation=\"relu\")(x3)            # 4th hidden layer with 25 neurons\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x4)      # Output layer with 1 neuron and sigmoid to force binary\n",
    "\n",
    "# Create the model\n",
    "NN = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "#Initialize the NN\n",
    "NN.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.F1Score()],\n",
    "    weighted_metrics=[\"f1_score\"],  # Weight the F1 metric higher\n",
    ")\n",
    "\n",
    "# Define the callbacks for early stopping\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',     # Use F1 for early stopping\n",
    "        mode='max',                 # We want to \"maximize\" the F1 score  \n",
    "        patience=4,                 # If theres no improvements after 4 epochs, stop\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define the weights for the classes\n",
    "weight_for_0 = len(y_train) / (2.0 * (y_train == 0).sum())\n",
    "weight_for_1 = len(y_train) / (2.0 * (y_train == 1).sum())\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "#Train the NN\n",
    "history = NN.fit(\n",
    "    X_train,y_train,                    # Trainin data as input and expected output\n",
    "    validation_data = (X_val,y_val),    # Data for validation\n",
    "    epochs = 10,                        # Number of iterations over the entire dataset\n",
    "    batch_size = 1024,                  # Number of samples per gradient update\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "output_path = \"models/nn_classifier11_f1_val_0.9214.keras\"\n",
    "NN.save(output_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models \n",
    "## nn_classifier1_f1_val_0.9328.keras - runtime: 16,6 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 256 bach size\n",
    "-   val_accuracy: 0.9695 - val_auc: 0.9820 - val_f1_score: 0.9328 - val_loss: 0.1359 - val_precision: 0.9732 - val_recall: 0.9870\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0917\n",
    "* Test Accuracy: 0.9685\n",
    "* Test Precision: 0.9696\n",
    "* Test Recall: 0.9895\n",
    "* Test AUC: 0.9901\n",
    "* Test F1 Score: 0.8937\n",
    "\n",
    "## nn_classifier2_f1_val_0.9304.keras - runtime: 10,75 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9690 - val_auc_2: 0.9802 - val_f1_score: 0.9304 - val_loss: 0.1467 - val_precision_2: 0.9744 - val_recall_2: 0.9851\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0903\n",
    "* Test Accuracy: 0.9685\n",
    "* Test Precision: 0.9751\n",
    "* Test Recall: 0.9836\n",
    "* Test AUC: 0.9908\n",
    "* Test F1 Score: 0.8951\n",
    "\n",
    "## nn_classifier3_f1_val_0.9228.keras - runtime: 10,5 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9703 - val_auc_4: 0.9828 - val_f1_score: 0.9228 - val_loss: 0.1242 - val_precision_4: 0.9752 - val_recall_4: 0.9860\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0919\n",
    "* Test Accuracy: 0.9689\n",
    "* Test Precision: 0.9729\n",
    "* Test Recall: 0.9866\n",
    "* Test AUC: 0.9897\n",
    "* Test F1 Score: 0.8829\n",
    "\n",
    "## nn_classifier4_f1_val_0.9292\n",
    "-   5000 (ReLu)(0.4 dropoff),1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9697 - val_auc_5: 0.9818 - val_f1_score: 0.9292 - val_loss: 0.1331 - val_precision_5: 0.9750 - val_recall_5: 0.9854\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0919\n",
    "* Test Accuracy: 0.9689\n",
    "* Test Precision: 0.9729\n",
    "* Test Recall: 0.9866\n",
    "* Test AUC: 0.9897\n",
    "* Test F1 Score: 0.8829\n",
    "\n",
    "## nn_classifier5_f1_val_0.9292 - runtime 10,75\n",
    "-   Alternate model\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9692 - val_auc_6: 0.9815 - val_f1_score: 0.9271 - val_loss: 0.1377 - val_precision_6: 0.9742 - val_recall_6: 0.9856\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0919\n",
    "* Test Accuracy: 0.9684\n",
    "* Test Precision: 0.9737\n",
    "* Test Recall: 0.9849\n",
    "* Test AUC: 0.9901\n",
    "* Test F1 Score: 0.8983\n",
    "\n",
    "## nn_classifier6_f1_val_0.916 - 16 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 256 bach size\n",
    "\n",
    "## models/nn_classifier7_f1_val_0.9266.keras - 10 min - NO unreliable\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0928\n",
    "* Test Accuracy: 0.9678\n",
    "* Test Precision: 0.9744\n",
    "* Test Recall: 0.9827\n",
    "* Test AUC: 0.9901\n",
    "* Test F1 Score: 0.8861\n",
    "\n",
    "## nn_classifier8_f1_val_0.6775 - 5 min - ONLY reliable and fake\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   7 epochs, 1024 bach size\n",
    "\n",
    "### Model evaluation key data\n",
    "* Test Loss: 17.5059\n",
    "* Test Accuracy: 0.3997\n",
    "* Test Precision: 0.7829\n",
    "* Test Recall: 0.2741\n",
    "* Test AUC: 0.5415\n",
    "* Test F1 Score: 0.6991\n",
    "\n",
    "## nn_classifier9_f1_val_0.9214 - 11 min - NO unreliable\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   7 epochs, 1024 bach size\n",
    "\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0991\n",
    "* Test Accuracy: 0.9665\n",
    "* Test Precision: 0.9716\n",
    "* Test Recall: 0.9840\n",
    "* Test AUC: 0.9896\n",
    "* Test F1 Score: 0.8901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "input_path = \"models/nn_alt_classifier2_f1_test_0.9234.keras\"\n",
    "NN = tf.keras.models.load_model(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2714/2714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - f1_score: 0.9257 - loss: 0.1946 - precision: 0.9701 - recall: 0.9806 - weighted_f1_score: 0.9257\n",
      "* Test Loss: 0.1939\n",
      "* Test Accuracy: 0.9712\n",
      "* Test Precision: 0.9794\n",
      "* Test Recall: 0.9276\n",
      "* Test AUC: 0.9276\n",
      "* Test F1 Score: 0.9276\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_results = NN.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"* Test Loss: {test_results[0]:.4f}\")\n",
    "print(f\"* Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"* Test Precision: {test_results[2]:.4f}\")\n",
    "print(f\"* Test Recall: {test_results[3]:.4f}\")\n",
    "print(f\"* Test AUC: {test_results[4]:.4f}\")\n",
    "print(f\"* Test F1 Score: {test_results[5]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
