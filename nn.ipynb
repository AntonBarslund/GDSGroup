{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/GDSvenv/lib/python3.12/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_47491/1965339663.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'bias' 'rumor'\n",
      " 'clickbait' 'hate' 'junksci']\n",
      "[1 0]\n",
      "type\n",
      "1    649783\n",
      "0    218564\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')\n",
    "\n",
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,  \"clickbait\": 1,   \"political\": 1,\n",
    "             \"reliable\": 0} #, \"unreliable\": 1\n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].unique())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].unique())\n",
    "print(df[\"type\"].value_counts())\n",
    "\n",
    "y = df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = df[\"content\"]\n",
    "# vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # Increase feature count\n",
    ")\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, y, \n",
    "    test_size=0.2,\n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5,\n",
    "    shuffle=False)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)  # fit and transform on train\n",
    "X_val = vectorizer.transform(X_val)          # only transform on val\n",
    "X_test = vectorizer.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (694677, 10000)\n",
      "X_val.shape: (86835, 10000)\n",
      "X_test.shape: (86835, 10000)\n",
      "\n",
      "y_train.shape: (694677,)\n",
      "y_val.shape: (86835,)\n",
      "y_test.shape: (86835,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_val.shape: {X_val.shape}\")\n",
    "print(f\"X_test.shape: {X_test.shape}\")\n",
    "print()\n",
    "\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"y_val.shape: {y_val.shape}\")\n",
    "print(f\"y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 218ms/step - f1_score: 0.8589 - loss: 0.2114 - precision_3: 0.9653 - recall_3: 0.9186 - weighted_f1_score: 0.6673 - val_f1_score: 0.8457 - val_loss: 0.1183 - val_precision_3: 0.9678 - val_recall_3: 0.9735 - val_weighted_f1_score: 0.8457\n",
      "Epoch 2/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 216ms/step - f1_score: 0.8595 - loss: 0.0979 - precision_3: 0.9864 - recall_3: 0.9651 - weighted_f1_score: 0.6685 - val_f1_score: 0.8468 - val_loss: 0.1102 - val_precision_3: 0.9757 - val_recall_3: 0.9689 - val_weighted_f1_score: 0.8468\n",
      "Epoch 3/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 217ms/step - f1_score: 0.8619 - loss: 0.0479 - precision_3: 0.9944 - recall_3: 0.9815 - weighted_f1_score: 0.6730 - val_f1_score: 0.8472 - val_loss: 0.1186 - val_precision_3: 0.9760 - val_recall_3: 0.9683 - val_weighted_f1_score: 0.8472\n",
      "Epoch 4/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 216ms/step - f1_score: 0.8656 - loss: 0.0268 - precision_3: 0.9970 - recall_3: 0.9901 - weighted_f1_score: 0.6798 - val_f1_score: 0.8555 - val_loss: 0.1396 - val_precision_3: 0.9743 - val_recall_3: 0.9752 - val_weighted_f1_score: 0.8555\n",
      "Epoch 5/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 216ms/step - f1_score: 0.8697 - loss: 0.0189 - precision_3: 0.9978 - recall_3: 0.9931 - weighted_f1_score: 0.6874 - val_f1_score: 0.8670 - val_loss: 0.1513 - val_precision_3: 0.9758 - val_recall_3: 0.9718 - val_weighted_f1_score: 0.8670\n",
      "Epoch 6/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 215ms/step - f1_score: 0.8794 - loss: 0.0141 - precision_3: 0.9984 - recall_3: 0.9951 - weighted_f1_score: 0.7061 - val_f1_score: 0.8719 - val_loss: 0.1556 - val_precision_3: 0.9736 - val_recall_3: 0.9759 - val_weighted_f1_score: 0.8719\n",
      "Epoch 7/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 214ms/step - f1_score: 0.8826 - loss: 0.0119 - precision_3: 0.9986 - recall_3: 0.9961 - weighted_f1_score: 0.7125 - val_f1_score: 0.8613 - val_loss: 0.1554 - val_precision_3: 0.9752 - val_recall_3: 0.9737 - val_weighted_f1_score: 0.8613\n",
      "Epoch 8/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 214ms/step - f1_score: 0.8830 - loss: 0.0103 - precision_3: 0.9988 - recall_3: 0.9966 - weighted_f1_score: 0.7132 - val_f1_score: 0.8720 - val_loss: 0.1688 - val_precision_3: 0.9717 - val_recall_3: 0.9797 - val_weighted_f1_score: 0.8720\n",
      "Epoch 9/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 217ms/step - f1_score: 0.8871 - loss: 0.0089 - precision_3: 0.9989 - recall_3: 0.9970 - weighted_f1_score: 0.7215 - val_f1_score: 0.8794 - val_loss: 0.1648 - val_precision_3: 0.9748 - val_recall_3: 0.9750 - val_weighted_f1_score: 0.8794\n",
      "Epoch 10/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 217ms/step - f1_score: 0.8911 - loss: 0.0081 - precision_3: 0.9991 - recall_3: 0.9974 - weighted_f1_score: 0.7295 - val_f1_score: 0.8875 - val_loss: 0.1789 - val_precision_3: 0.9742 - val_recall_3: 0.9772 - val_weighted_f1_score: 0.8875\n"
     ]
    }
   ],
   "source": [
    "# Try a more complex architecture with residual connections\n",
    "weight_for_0 = len(y_train) / (2.0 * (y_train == 0).sum())\n",
    "weight_for_1 = len(y_train) / (2.0 * (y_train == 1).sum())\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "input_layer = Input(shape=(X_train.shape[1],), sparse=True)\n",
    "x1 = layers.Dense(1500, activation=\"relu\")(input_layer)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.Dropout(0.4)(x1)\n",
    "\n",
    "x2 = layers.Dense(750, activation=\"relu\")(x1)\n",
    "x2 = layers.BatchNormalization()(x2)\n",
    "x2 = layers.Dropout(0.3)(x2)\n",
    "\n",
    "# Residual connection\n",
    "x3 = layers.concatenate([x1, x2])\n",
    "x3 = layers.Dense(300, activation=\"relu\")(x3)\n",
    "x3 = layers.BatchNormalization()(x3)\n",
    "x3 = layers.Dropout(0.2)(x3)\n",
    "\n",
    "x4 = layers.Dense(100, activation=\"relu\")(x3)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x4)\n",
    "\n",
    "NN = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "#Initialize the NN\n",
    "NN.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.F1Score()],\n",
    "    weighted_metrics=[\"f1_score\"],  # Weight the F1 metric\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        mode='max', \n",
    "        patience=4,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "#Train the NN\n",
    "history = NN.fit(\n",
    "    X_train,y_train,\n",
    "    validation_data = (X_val,y_val),\n",
    "    epochs = 10,\n",
    "    batch_size = 1024,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "output_path = \"models/nn_alt_classifier2_f1_val_0.9214.keras\"\n",
    "NN.save(output_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 136ms/step - f1_score: 0.8585 - loss: 0.2340 - precision_4: 0.9650 - recall_4: 0.9024 - weighted_f1_score: 0.6666 - val_f1_score: 0.8497 - val_loss: 0.1465 - val_precision_4: 0.9781 - val_recall_4: 0.9470 - val_weighted_f1_score: 0.8497\n",
      "Epoch 2/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 141ms/step - f1_score: 0.8676 - loss: 0.1014 - precision_4: 0.9860 - recall_4: 0.9664 - weighted_f1_score: 0.6835 - val_f1_score: 0.8589 - val_loss: 0.1293 - val_precision_4: 0.9817 - val_recall_4: 0.9518 - val_weighted_f1_score: 0.8589\n",
      "Epoch 3/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 139ms/step - f1_score: 0.8844 - loss: 0.0424 - precision_4: 0.9951 - recall_4: 0.9857 - weighted_f1_score: 0.7162 - val_f1_score: 0.8771 - val_loss: 0.1230 - val_precision_4: 0.9781 - val_recall_4: 0.9645 - val_weighted_f1_score: 0.8771\n",
      "Epoch 4/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 138ms/step - f1_score: 0.9007 - loss: 0.0222 - precision_4: 0.9976 - recall_4: 0.9925 - weighted_f1_score: 0.7494 - val_f1_score: 0.8886 - val_loss: 0.1322 - val_precision_4: 0.9773 - val_recall_4: 0.9684 - val_weighted_f1_score: 0.8886\n",
      "Epoch 5/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 138ms/step - f1_score: 0.9077 - loss: 0.0157 - precision_4: 0.9982 - recall_4: 0.9948 - weighted_f1_score: 0.7643 - val_f1_score: 0.9006 - val_loss: 0.1492 - val_precision_4: 0.9735 - val_recall_4: 0.9749 - val_weighted_f1_score: 0.9006\n",
      "Epoch 6/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 137ms/step - f1_score: 0.9183 - loss: 0.0117 - precision_4: 0.9986 - recall_4: 0.9963 - weighted_f1_score: 0.7874 - val_f1_score: 0.9118 - val_loss: 0.1651 - val_precision_4: 0.9750 - val_recall_4: 0.9742 - val_weighted_f1_score: 0.9118\n",
      "Epoch 7/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 138ms/step - f1_score: 0.9249 - loss: 0.0090 - precision_4: 0.9989 - recall_4: 0.9972 - weighted_f1_score: 0.8023 - val_f1_score: 0.9060 - val_loss: 0.1555 - val_precision_4: 0.9739 - val_recall_4: 0.9759 - val_weighted_f1_score: 0.9060\n",
      "Epoch 8/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 138ms/step - f1_score: 0.9287 - loss: 0.0083 - precision_4: 0.9990 - recall_4: 0.9974 - weighted_f1_score: 0.8112 - val_f1_score: 0.9188 - val_loss: 0.1657 - val_precision_4: 0.9741 - val_recall_4: 0.9764 - val_weighted_f1_score: 0.9188\n",
      "Epoch 9/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 138ms/step - f1_score: 0.9323 - loss: 0.0075 - precision_4: 0.9991 - recall_4: 0.9978 - weighted_f1_score: 0.8194 - val_f1_score: 0.9220 - val_loss: 0.1710 - val_precision_4: 0.9720 - val_recall_4: 0.9784 - val_weighted_f1_score: 0.9220\n",
      "Epoch 10/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 139ms/step - f1_score: 0.9356 - loss: 0.0070 - precision_4: 0.9991 - recall_4: 0.9979 - weighted_f1_score: 0.8273 - val_f1_score: 0.9226 - val_loss: 0.1765 - val_precision_4: 0.9741 - val_recall_4: 0.9752 - val_weighted_f1_score: 0.9226\n"
     ]
    }
   ],
   "source": [
    "# Standard model\n",
    "# Standard model with corrected tensor flow\n",
    "weight_for_0 = len(y_train) / (2.0 * (y_train == 0).sum())\n",
    "weight_for_1 = len(y_train) / (2.0 * (y_train == 1).sum())\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "input_layer = Input(shape=(X_train.shape[1],), sparse=True)\n",
    "x1 = layers.Dense(1000, activation=\"relu\")(input_layer)\n",
    "x1 = layers.Dropout(0.4)(x1)\n",
    "x2 = layers.Dense(500, activation=\"relu\")(x1)\n",
    "x2 = layers.Dropout(0.2)(x2)\n",
    "x3 = layers.Dense(100, activation=\"relu\")(x2)\n",
    "x3 = layers.Dropout(0.2)(x3)\n",
    "x4 = layers.Dense(25, activation=\"relu\")(x3)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x4)\n",
    "\n",
    "NN = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "\n",
    "#Initialize the NN\n",
    "NN.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.F1Score()],\n",
    "    weighted_metrics=[\"f1_score\"],  # Weight the F1 metric\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        mode='max', \n",
    "        patience=4,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "#Train the NN\n",
    "history = NN.fit(\n",
    "    X_train,y_train,\n",
    "    validation_data = (X_val,y_val),\n",
    "    epochs = 10,\n",
    "    batch_size = 1024,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "output_path = \"models/nn_classifier11_f1_val_0.9214.keras\"\n",
    "NN.save(output_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve analysis\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learinig curves analysis\n",
    "# Plot training & validation accuracy/loss values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models \n",
    "## nn_classifier1_f1_val_0.9328.keras - runtime: 16,6 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 256 bach size\n",
    "-   val_accuracy: 0.9695 - val_auc: 0.9820 - val_f1_score: 0.9328 - val_loss: 0.1359 - val_precision: 0.9732 - val_recall: 0.9870\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0917\n",
    "* Test Accuracy: 0.9685\n",
    "* Test Precision: 0.9696\n",
    "* Test Recall: 0.9895\n",
    "* Test AUC: 0.9901\n",
    "* Test F1 Score: 0.8937\n",
    "\n",
    "## nn_classifier2_f1_val_0.9304.keras - runtime: 10,75 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9690 - val_auc_2: 0.9802 - val_f1_score: 0.9304 - val_loss: 0.1467 - val_precision_2: 0.9744 - val_recall_2: 0.9851\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0903\n",
    "* Test Accuracy: 0.9685\n",
    "* Test Precision: 0.9751\n",
    "* Test Recall: 0.9836\n",
    "* Test AUC: 0.9908\n",
    "* Test F1 Score: 0.8951\n",
    "\n",
    "## nn_classifier3_f1_val_0.9228.keras - runtime: 10,5 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9703 - val_auc_4: 0.9828 - val_f1_score: 0.9228 - val_loss: 0.1242 - val_precision_4: 0.9752 - val_recall_4: 0.9860\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0919\n",
    "* Test Accuracy: 0.9689\n",
    "* Test Precision: 0.9729\n",
    "* Test Recall: 0.9866\n",
    "* Test AUC: 0.9897\n",
    "* Test F1 Score: 0.8829\n",
    "\n",
    "## nn_classifier4_f1_val_0.9292\n",
    "-   5000 (ReLu)(0.4 dropoff),1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9697 - val_auc_5: 0.9818 - val_f1_score: 0.9292 - val_loss: 0.1331 - val_precision_5: 0.9750 - val_recall_5: 0.9854\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0919\n",
    "* Test Accuracy: 0.9689\n",
    "* Test Precision: 0.9729\n",
    "* Test Recall: 0.9866\n",
    "* Test AUC: 0.9897\n",
    "* Test F1 Score: 0.8829\n",
    "\n",
    "## nn_classifier5_f1_val_0.9292 - runtime 10,75\n",
    "-   Alternate model\n",
    "-   5 epochs, 512 bach size\n",
    "-   val_accuracy: 0.9692 - val_auc_6: 0.9815 - val_f1_score: 0.9271 - val_loss: 0.1377 - val_precision_6: 0.9742 - val_recall_6: 0.9856\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0919\n",
    "* Test Accuracy: 0.9684\n",
    "* Test Precision: 0.9737\n",
    "* Test Recall: 0.9849\n",
    "* Test AUC: 0.9901\n",
    "* Test F1 Score: 0.8983\n",
    "\n",
    "## nn_classifier6_f1_val_0.916 - 16 min\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 256 bach size\n",
    "\n",
    "## models/nn_classifier7_f1_val_0.9266.keras - 10 min - NO unreliable\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   5 epochs, 512 bach size\n",
    "\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0928\n",
    "* Test Accuracy: 0.9678\n",
    "* Test Precision: 0.9744\n",
    "* Test Recall: 0.9827\n",
    "* Test AUC: 0.9901\n",
    "* Test F1 Score: 0.8861\n",
    "\n",
    "## nn_classifier8_f1_val_0.6775 - 5 min - ONLY reliable and fake\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   7 epochs, 1024 bach size\n",
    "\n",
    "### Model evaluation key data\n",
    "* Test Loss: 17.5059\n",
    "* Test Accuracy: 0.3997\n",
    "* Test Precision: 0.7829\n",
    "* Test Recall: 0.2741\n",
    "* Test AUC: 0.5415\n",
    "* Test F1 Score: 0.6991\n",
    "\n",
    "## nn_classifier9_f1_val_0.9214 - 11 min - NO unreliable\n",
    "-   1000 (ReLu)(0.4 dropoff), 500 (ReLu)(0.2 dropoff), 100 (ReLu)(0.2 dropoff), 50 (ReLu)\n",
    "-   7 epochs, 1024 bach size\n",
    "\n",
    "### Model evaluation key data\n",
    "* Test Loss: 0.0991\n",
    "* Test Accuracy: 0.9665\n",
    "* Test Precision: 0.9716\n",
    "* Test Recall: 0.9840\n",
    "* Test AUC: 0.9896\n",
    "* Test F1 Score: 0.8901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "input_path = \"models/nn_classifier11_f1_val_0.9214.keras\"\n",
    "NN = tf.keras.models.load_model(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2714/2714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - f1_score: 0.9214 - loss: 0.1774 - precision_4: 0.9714 - recall_4: 0.9766 - weighted_f1_score: 0.9214\n",
      "* Test Loss: 0.1784\n",
      "* Test Accuracy: 0.9728\n",
      "* Test Precision: 0.9755\n",
      "* Test Recall: 0.9234\n",
      "* Test AUC: 0.9234\n",
      "* Test F1 Score: 0.9234\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_results = NN.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"* Test Loss: {test_results[0]:.4f}\")\n",
    "print(f\"* Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"* Test Precision: {test_results[2]:.4f}\")\n",
    "print(f\"* Test Recall: {test_results[3]:.4f}\")\n",
    "print(f\"* Test AUC: {test_results[4]:.4f}\")\n",
    "print(f\"* Test F1 Score: {test_results[5]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
