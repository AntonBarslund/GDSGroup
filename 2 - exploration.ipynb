{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "import swifter\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Required resources; download once\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_16048/1234656882.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:00<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1639414 unique words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process in chunks to avoid memory issues\n",
    "def create_vocab_optimized(df, chunk_size=10000):\n",
    "    vocab = Counter()\n",
    "    \n",
    "    # Process DataFrame in chunks\n",
    "    for i in tqdm(range(0, len(df), chunk_size)):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        \n",
    "        # Update vocabulary with words from this chunk\n",
    "        for content in chunk['content']:\n",
    "            try:\n",
    "                word_list = ast.literal_eval(content)\n",
    "                vocab.update(word_list)\n",
    "            except (ValueError, SyntaxError):\n",
    "                # Skip malformed entries\n",
    "                continue\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Replace the original vocabulary creation\n",
    "vocab = create_vocab_optimized(df)\n",
    "print(f\"Vocabulary size: {len(vocab)} unique words\")\n",
    "\n",
    "# For saving top words\n",
    "shortend_vocab = dict(vocab.most_common(10000))\n",
    "with open('10000vocab.json', 'w') as f:\n",
    "    json.dump(shortend_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 995/995 [09:12<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (995000, 10000)\n",
      "Labels shape: (995000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_features_optimized(df, vocab_words, chunk_size=1000):\n",
    "    # Get the list of words we care about (fixed size vocabulary)\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "    \n",
    "    # Create a sparse matrix for features\n",
    "    features = lil_matrix((len(df), len(vocab_words)), dtype=np.int32)\n",
    "    labels = np.zeros(len(df), dtype=np.int8)\n",
    "    \n",
    "    # Label mapping\n",
    "    label_map = {\n",
    "        \"fake\": 1, \"satire\": 1, \"conspiracy\": 1, \"unreliable\": 1, \"bias\": 1, \n",
    "        \"rumor\": 1, \"junksci\": 1, \"hate\": 1, \"reliable\": 0, \"clickbait\": 0, \"political\": 0\n",
    "    }\n",
    "    \n",
    "    # Process in chunks\n",
    "    for i in tqdm(range(0, len(df), chunk_size)):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        \n",
    "        for j, (_, row) in enumerate(chunk.iterrows()):\n",
    "            absolute_idx = i + j\n",
    "            \n",
    "            # Set label\n",
    "            labels[absolute_idx] = label_map.get(row['type'], 0)\n",
    "            \n",
    "            # Process content\n",
    "            try:\n",
    "                word_list = ast.literal_eval(row['content'])\n",
    "                # Count words that are in our vocabulary\n",
    "                for word in word_list:\n",
    "                    if word in word_to_index:\n",
    "                        features[absolute_idx, word_to_index[word]] += 1\n",
    "            except (ValueError, SyntaxError):\n",
    "                continue\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Get top N words from vocabulary\n",
    "top_words = list(shortend_vocab.keys())\n",
    "features, labels = create_features_optimized(df, top_words)\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the processed fake dataset and look into the parts of speech within this subset, creating a new dataset with the analyzed result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the DataFrame to include only rows where the 'type' column is 'fake' or 'reliable'...\n",
      "Filtering complete! The filtered DataFrame contains 104883 rows.\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame\n",
    "print(\"Filtering the DataFrame to include only rows where the 'type' column is 'fake' or 'reliable'...\")\n",
    "filtered_df_fake = df[df['type'].isin(['fake'])]\n",
    "filtered_df_reliable = df[df['type'].isin(['reliable'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pos_distribution(df, column_name='content', chunk_size=1000):\n",
    "    # Initialize counters\n",
    "    pos_counts = Counter()\n",
    "    processed_rows = 0\n",
    "    \n",
    "    # POS Mapping Dictionary\n",
    "    pos_mapping = {\n",
    "        \"CC\": \"Conjunction (coordinating)\", \"CD\": \"Numeral (cardinal)\", \"DT\": \"Determiner\",\n",
    "        \"EX\": \"Existential 'there'\", \"IN\": \"Preposition or subordinating conjunction\",\n",
    "        \"JJ\": \"Adjective\", \"JJR\": \"Adjective (comparative)\", \"JJS\": \"Adjective (superlative)\",\n",
    "        \"LS\": \"List item marker\", \"MD\": \"Modal auxiliary\", \"NN\": \"Noun (singular/mass)\",\n",
    "        \"NNS\": \"Noun (plural)\", \"NNP\": \"Proper Noun (singular)\", \"NNPS\": \"Proper Noun (plural)\",\n",
    "        \"PDT\": \"Pre-determiner\", \"POS\": \"Genitive marker ('s)\", \"PRP\": \"Pronoun (personal)\",\n",
    "        \"PRP$\": \"Pronoun (possessive)\", \"RB\": \"Adverb\", \"RBR\": \"Adverb (comparative)\",\n",
    "        \"RBS\": \"Adverb (superlative)\", \"RP\": \"Particle\", \"TO\": \"To (preposition/infinitive marker)\",\n",
    "        \"UH\": \"Interjection\", \"VB\": \"Verb (base form)\", \"VBD\": \"Verb (past tense)\",\n",
    "        \"VBG\": \"Verb (present participle/gerund)\", \"VBN\": \"Verb (past participle)\",\n",
    "        \"VBP\": \"Verb (present, non-3rd person singular)\", \"VBZ\": \"Verb (present, 3rd person singular)\",\n",
    "        \"WDT\": \"WH-determiner\", \"WP\": \"WH-pronoun\", \"WRB\": \"WH-adverb\"\n",
    "    }\n",
    "    \n",
    "    # Process in chunks\n",
    "    for i in tqdm(range(0, len(df), chunk_size)):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        \n",
    "        for text in chunk[column_name].fillna('').astype(str):\n",
    "            try:\n",
    "                # Skip empty text\n",
    "                if not text.strip():\n",
    "                    continue\n",
    "                \n",
    "                # Tokenize and get POS tags\n",
    "                tokens = word_tokenize(text)\n",
    "                pos_tags = pos_tag(tokens)\n",
    "                \n",
    "                # Update POS counts\n",
    "                for _, tag in pos_tags:\n",
    "                    readable_tag = pos_mapping.get(tag, \"Other\")\n",
    "                    pos_counts[readable_tag] += 1\n",
    "                \n",
    "                processed_rows += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Processed {processed_rows} rows\")\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    df_counts = pd.DataFrame(pos_counts.items(), columns=['POS', 'Count'])\n",
    "    df_counts = df_counts.sort_values('Count', ascending=False)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(df_counts['POS'], df_counts['Count'])\n",
    "    plt.xlabel(\"Part of Speech\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"POS Distribution\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_counts\n",
    "\n",
    "# Example usage:\n",
    "# pos_counts_fake = analyze_pos_distribution(filtered_df_fake, column_name='content')\n",
    "# pos_counts_reliable = analyze_pos_distribution(filtered_df_reliable, column_name='content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the relationship between the total amount of words and the total amount of unique words.\n",
    "#### Finding the ratio of unique words per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [00:06<00:00, 159909.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in dataset: 263245932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [00:46<00:00, 21196.93it/s]\n",
      "100%|██████████| 995000/995000 [00:00<00:00, 7307943.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in dataset: 1726502\n",
      "Unique words per word: 0.6558513504398616\n"
     ]
    }
   ],
   "source": [
    "# Total amount of word in content\n",
    "total_words = df[\"content\"].swifter.progress_bar(True).apply(lambda x: len(str(x).split())).sum()\n",
    "print(\"Total words in dataset:\", total_words)\n",
    "\n",
    "# Function to extract unique words from a text\n",
    "def extract_unique_words(text):\n",
    "    return set(str(text).split())\n",
    "\n",
    "# Apply function efficiently using swifter\n",
    "unique_word_sets = df[\"content\"].swifter.progress_bar(True).apply(extract_unique_words)\n",
    "\n",
    "# Combine all sets and count unique words\n",
    "total_unique_words = len(set().union(*tqdm(unique_word_sets)))\n",
    "\n",
    "print(\"Total unique words in dataset:\", total_unique_words)\n",
    "\n",
    "# Calculate the percentage of unique words per word\n",
    "print(\"Unique words per word:\", (total_unique_words / total_words) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
