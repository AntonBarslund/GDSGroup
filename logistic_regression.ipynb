{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import collections\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/news_sample_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_71955/1234656882.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "reliable      218564\n",
      "political     194518\n",
      "bias          133232\n",
      "fake          104883\n",
      "conspiracy     97314\n",
      "rumor          56445\n",
      "unreliable     35332\n",
      "clickbait      27412\n",
      "junksci        14040\n",
      "satire         13160\n",
      "hate            8779\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "1    463185\n",
      "0    440494\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"unreliable\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,\n",
    "             \"reliable\": 0,  \"clickbait\": 0,   \"political\": 0}\n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].value_counts())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 0 is minority\n",
      "\n",
      "Balanced distribution:\n",
      "type\n",
      "1    440494\n",
      "0    440494\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Balancing the dataset\n",
    "# Separate the two classes\n",
    "type_0 = df[df['type'] == 0]\n",
    "type_1 = df[df['type'] == 1]\n",
    "\n",
    "# Check class sizes\n",
    "count_0 = len(type_0)\n",
    "count_1 = len(type_1)\n",
    "\n",
    "# Identify minority and majority classes\n",
    "if count_0 < count_1:\n",
    "    # Type 0 is minority\n",
    "    min_count = count_0\n",
    "    majority_class = type_1\n",
    "    minority_class = type_0\n",
    "    print(\"Type 0 is minority\")\n",
    "else:\n",
    "    # Type 1 is minority\n",
    "    min_count = count_1\n",
    "    majority_class = type_0\n",
    "    minority_class = type_1\n",
    "    print(\"Type 1 is minority\")\n",
    "\n",
    "# Undersample majority class\n",
    "majority_undersampled = majority_class.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine the undersampled majority with minority\n",
    "balanced_df = pd.concat([majority_undersampled, minority_class])\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"\\nBalanced distribution:\")\n",
    "print(balanced_df['type'].value_counts())\n",
    "\n",
    "# Save the balanced dataset\n",
    "balanced_df.to_csv('data/995,000_row_balanced.csv', index=False)\n",
    "balanced_df.sort_values(by='scraped_at', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.48943801859271957\n",
      "LogisticRegression accuracy:  0.5105619814072805\n",
      "LogisticRegression F1 score:  0.001620783069763134\n"
     ]
    }
   ],
   "source": [
    "# Label all to either 1 or 0\n",
    "labels = balanced_df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = balanced_df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, labels, \n",
    "    test_size=0.2,  \n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    shuffle=False)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)  \n",
    "X_val = vectorizer.transform(X_val)          \n",
    "X_test = vectorizer.transform(X_test)  \n",
    "\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.48943801859271957\n",
      "LogisticRegression accuracy:  0.5105619814072805\n",
      "LogisticRegression F1 score:  0.001620783069763134\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 440456, 0: 264334})\n",
      "Counter({0: 88061, 1: 38})\n",
      "Counter({0: 88099})\n",
      "Counter({0: 440494, 1: 440494})\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter(y_train)\n",
    "print(counter)\n",
    "counter = collections.Counter(y_val)\n",
    "print(counter)\n",
    "counter = collections.Counter(y_test)\n",
    "print(counter)\n",
    "counter = collections.Counter(labels)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.13151779390934845\n",
      "LogisticRegression accuracy:  0.8684822060906515\n",
      "LogisticRegression F1 score:  0.8797539432814981\n"
     ]
    }
   ],
   "source": [
    "## Test unblananaced data\n",
    "# Label all to either 1 or 0\n",
    "labels = df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    shuffle=False)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if only reliable is reliabel and unreliable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_71955/122759731.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'clickbait' 'hate' 'junksci']\n",
      "[1 0]\n",
      "type\n",
      "1    685115\n",
      "0    218564\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')\n",
    "\n",
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"unreliable\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,  \"clickbait\": 1,   \"political\": 1,\n",
    "             \"reliable\": 0}\n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].unique())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].unique())\n",
    "print(df[\"type\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 0 is minority\n",
      "\n",
      "Balanced distribution:\n",
      "type\n",
      "1    218564\n",
      "0    218564\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Balancing the dataset\n",
    "# Separate the two classes\n",
    "type_0 = df[df['type'] == 0]\n",
    "type_1 = df[df['type'] == 1]\n",
    "\n",
    "# Check class sizes\n",
    "count_0 = len(type_0)\n",
    "count_1 = len(type_1)\n",
    "\n",
    "# Identify minority and majority classes\n",
    "if count_0 < count_1:\n",
    "    # Type 0 is minority\n",
    "    min_count = count_0\n",
    "    majority_class = type_1\n",
    "    minority_class = type_0\n",
    "    print(\"Type 0 is minority\")\n",
    "else:\n",
    "    # Type 1 is minority\n",
    "    min_count = count_1\n",
    "    majority_class = type_0\n",
    "    minority_class = type_1\n",
    "    print(\"Type 1 is minority\")\n",
    "\n",
    "# Undersample majority class\n",
    "majority_undersampled = majority_class.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine the undersampled majority with minority\n",
    "balanced_df = pd.concat([majority_undersampled, minority_class])\n",
    "\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"\\nBalanced distribution:\")\n",
    "print(balanced_df['type'].value_counts())\n",
    "\n",
    "# Save the balanced dataset\n",
    "balanced_df.to_csv('data/995,000_row_balanced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.057465031869688384\n",
      "LogisticRegression accuracy:  0.9425349681303116\n",
      "LogisticRegression F1 score:  0.962696377389393\n"
     ]
    }
   ],
   "source": [
    "## Test unblananaced data\n",
    "# Label all to either 1 or 0\n",
    "labels = df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    shuffle=False)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.1273534188914053\n",
      "LogisticRegression accuracy:  0.8726465811085947\n",
      "LogisticRegression F1 score:  0.0\n"
     ]
    }
   ],
   "source": [
    "## Test balanced data \n",
    "\n",
    "# Label all to either 1 or 0\n",
    "labels = balanced_df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = balanced_df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    shuffle=False)\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliable and politcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_71955/171397139.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'clickbait' 'hate' 'junksci']\n",
      "[0 1]\n",
      "type\n",
      "1    490597\n",
      "0    413082\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')\n",
    "\n",
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"unreliable\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,  \"clickbait\": 1,\n",
    "             \"reliable\": 0,   \"political\": 0}\n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].unique())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].unique())\n",
    "print(df[\"type\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 0 is minority\n",
      "\n",
      "Balanced distribution:\n",
      "type\n",
      "1    413082\n",
      "0    413082\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Balancing the dataset\n",
    "# Separate the two classes\n",
    "type_0 = df[df['type'] == 0]\n",
    "type_1 = df[df['type'] == 1]\n",
    "\n",
    "# Check class sizes\n",
    "count_0 = len(type_0)\n",
    "count_1 = len(type_1)\n",
    "\n",
    "# Identify minority and majority classes\n",
    "if count_0 < count_1:\n",
    "    # Type 0 is minority\n",
    "    min_count = count_0\n",
    "    majority_class = type_1\n",
    "    minority_class = type_0\n",
    "    print(\"Type 0 is minority\")\n",
    "else:\n",
    "    # Type 1 is minority\n",
    "    min_count = count_1\n",
    "    majority_class = type_0\n",
    "    minority_class = type_1\n",
    "    print(\"Type 1 is minority\")\n",
    "\n",
    "# Undersample majority class\n",
    "majority_undersampled = majority_class.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine the undersampled majority with minority\n",
    "balanced_df = pd.concat([majority_undersampled, minority_class])\n",
    "\n",
    "\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"\\nBalanced distribution:\")\n",
    "print(balanced_df['type'].value_counts())\n",
    "\n",
    "# Save the balanced dataset\n",
    "balanced_df.to_csv('data/995,000_row_balanced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.12999070467422097\n",
      "LogisticRegression accuracy:  0.5181037535410765\n",
      "LogisticRegression F1 score:  0.5914285178165988\n"
     ]
    }
   ],
   "source": [
    "## Test unblananaced data\n",
    "# Label all to either 1 or 0\n",
    "labels = df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    shuffle=False)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression accuracy:  0.7442989251476712\n",
      "LogisticRegression F1 score:  0.0\n"
     ]
    }
   ],
   "source": [
    "## Test balanced data \n",
    "\n",
    "# Label all to either 1 or 0\n",
    "labels = balanced_df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = balanced_df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    shuffle=False)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unrelaible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_71955/68872884.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'bias' 'rumor'\n",
      " 'clickbait' 'hate' 'junksci']\n",
      "[0 1]\n",
      "type\n",
      "1    455265\n",
      "0    413082\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')\n",
    "\n",
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,  \"clickbait\": 1,\n",
    "             \"reliable\": 0,   \"political\": 0}\n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].unique())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].unique())\n",
    "print(df[\"type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 0 is minority\n",
      "\n",
      "Balanced distribution:\n",
      "type\n",
      "1    413082\n",
      "0    413082\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Balancing the dataset\n",
    "# Separate the two classes\n",
    "type_0 = df[df['type'] == 0]\n",
    "type_1 = df[df['type'] == 1]\n",
    "\n",
    "# Check class sizes\n",
    "count_0 = len(type_0)\n",
    "count_1 = len(type_1)\n",
    "\n",
    "# Identify minority and majority classes\n",
    "if count_0 < count_1:\n",
    "    # Type 0 is minority\n",
    "    min_count = count_0\n",
    "    majority_class = type_1\n",
    "    minority_class = type_0\n",
    "    print(\"Type 0 is minority\")\n",
    "else:\n",
    "    # Type 1 is minority\n",
    "    min_count = count_1\n",
    "    majority_class = type_0\n",
    "    minority_class = type_1\n",
    "    print(\"Type 1 is minority\")\n",
    "\n",
    "# Undersample majority class\n",
    "majority_undersampled = majority_class.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine the undersampled majority with minority\n",
    "balanced_df = pd.concat([majority_undersampled, minority_class])\n",
    "\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"\\nBalanced distribution:\")\n",
    "print(balanced_df['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.14084182645246732\n",
      "LogisticRegression accuracy:  0.5013301088270858\n",
      "LogisticRegression F1 score:  0.5381317063805278\n"
     ]
    }
   ],
   "source": [
    "## Test unblananaced data\n",
    "# Label all to either 1 or 0\n",
    "labels = df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    shuffle=False)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.26055485620218843\n",
      "LogisticRegression accuracy:  0.7394451437978116\n",
      "LogisticRegression F1 score:  0.0\n"
     ]
    }
   ],
   "source": [
    "## Test balanced data \n",
    "\n",
    "# Label all to either 1 or 0\n",
    "labels = balanced_df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = balanced_df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    shuffle=False)\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only fake reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_71955/3971570677.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fake' 'reliable']\n",
      "[1 0]\n",
      "type\n",
      "0    218564\n",
      "1    104883\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')\n",
    "\n",
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1,\n",
    "             \"reliable\": 0}\n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].unique())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].unique())\n",
    "print(df[\"type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1 is minority\n",
      "\n",
      "Balanced distribution:\n",
      "type\n",
      "0    104883\n",
      "1    104883\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Balancing the dataset\n",
    "# Separate the two classes\n",
    "type_0 = df[df['type'] == 0]\n",
    "type_1 = df[df['type'] == 1]\n",
    "\n",
    "# Check class sizes\n",
    "count_0 = len(type_0)\n",
    "count_1 = len(type_1)\n",
    "\n",
    "# Identify minority and majority classes\n",
    "if count_0 < count_1:\n",
    "    # Type 0 is minority\n",
    "    min_count = count_0\n",
    "    majority_class = type_1\n",
    "    minority_class = type_0\n",
    "    print(\"Type 0 is minority\")\n",
    "else:\n",
    "    # Type 1 is minority\n",
    "    min_count = count_1\n",
    "    majority_class = type_0\n",
    "    minority_class = type_1\n",
    "    print(\"Type 1 is minority\")\n",
    "\n",
    "# Undersample majority class\n",
    "majority_undersampled = majority_class.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine the undersampled majority with minority\n",
    "balanced_df = pd.concat([majority_undersampled, minority_class])\n",
    "\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"\\nBalanced distribution:\")\n",
    "print(balanced_df['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.05527902303292626\n",
      "LogisticRegression accuracy:  0.5839233266347195\n",
      "LogisticRegression F1 score:  0.2974524953017331\n"
     ]
    }
   ],
   "source": [
    "## Test unblananaced data\n",
    "# Label all to either 1 or 0\n",
    "labels = df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    shuffle=False)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.08976498069314011\n",
      "LogisticRegression accuracy:  0.9102350193068599\n",
      "LogisticRegression F1 score:  0.953008410072122\n"
     ]
    }
   ],
   "source": [
    "## Test balanced data \n",
    "\n",
    "# Label all to either 1 or 0\n",
    "labels = balanced_df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = balanced_df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    shuffle=False)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    shuffle=False)\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
