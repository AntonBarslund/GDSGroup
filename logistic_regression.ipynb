{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/news_sample_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_13931/1234656882.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "reliable                      218564\n",
      "political                     194518\n",
      "bias                          133232\n",
      "fake                          104883\n",
      "conspiracy                     97314\n",
      "rumor                          56445\n",
      "unknown                        43534\n",
      "unreliable                     35332\n",
      "clickbait                      27412\n",
      "junksci                        14040\n",
      "satire                         13160\n",
      "hate                            8779\n",
      "2018-02-10 13:43:39.521661         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'clickbait' 'hate' 'junksci']\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"unreliable\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,\n",
    "             \"reliable\": 0,  \"clickbait\": 0,   \"political\": 0}\n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].unique())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 0 is minority\n",
      "\n",
      "Balanced distribution:\n",
      "type\n",
      "1    440494\n",
      "0    440494\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Balancing the dataset\n",
    "# Separate the two classes\n",
    "type_0 = df[df['type'] == 0]\n",
    "type_1 = df[df['type'] == 1]\n",
    "\n",
    "# Check class sizes\n",
    "count_0 = len(type_0)\n",
    "count_1 = len(type_1)\n",
    "\n",
    "# Identify minority and majority classes\n",
    "if count_0 < count_1:\n",
    "    # Type 0 is minority\n",
    "    min_count = count_0\n",
    "    majority_class = type_1\n",
    "    minority_class = type_0\n",
    "    print(\"Type 0 is minority\")\n",
    "else:\n",
    "    # Type 1 is minority\n",
    "    min_count = count_1\n",
    "    majority_class = type_0\n",
    "    minority_class = type_1\n",
    "    print(\"Type 1 is minority\")\n",
    "\n",
    "# Undersample majority class\n",
    "majority_undersampled = majority_class.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine the undersampled majority with minority\n",
    "balanced_df = pd.concat([majority_undersampled, minority_class])\n",
    "\n",
    "# Shuffle the data\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"\\nBalanced distribution:\")\n",
    "print(balanced_df['type'].value_counts())\n",
    "\n",
    "# Save the balanced dataset\n",
    "balanced_df.to_csv('data/995,000_row_balanced.csv', index=False)\n"
   ]
<<<<<<< Updated upstream
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.14100046538553218\n",
      "LogisticRegression accuracy:  0.8589995346144678\n",
      "LogisticRegression F1 score:  0.862946290657134\n"
     ]
    }
   ],
   "source": [
    "## Test balanced data \n",
    "\n",
    "# Label all to either 1 or 0\n",
    "labels = balanced_df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = balanced_df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=0)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=0) # TODO: fix this to not use random_state\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.14314801699716714\n",
      "LogisticRegression accuracy:  0.8568519830028328\n",
      "LogisticRegression F1 score:  0.8652275379229871\n"
     ]
    }
   ],
   "source": [
    "## Test unblananaced data\n",
    "# Label all to either 1 or 0\n",
    "labels = df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=0)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=0)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if only reliable is reliabeland unreliable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_13931/122759731.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'clickbait' 'hate' 'junksci']\n",
      "[1 0]\n",
      "type\n",
      "1    685115\n",
      "0    218564\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')\n",
    "\n",
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"unreliable\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,  \"clickbait\": 1,   \"political\": 1,\n",
    "             \"reliable\": 0}\n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].unique())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].unique())\n",
    "print(df[\"type\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 0 is minority\n",
      "\n",
      "Balanced distribution:\n",
      "type\n",
      "0    218564\n",
      "1    218564\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Balancing the dataset\n",
    "# Separate the two classes\n",
    "type_0 = df[df['type'] == 0]\n",
    "type_1 = df[df['type'] == 1]\n",
    "\n",
    "# Check class sizes\n",
    "count_0 = len(type_0)\n",
    "count_1 = len(type_1)\n",
    "\n",
    "# Identify minority and majority classes\n",
    "if count_0 < count_1:\n",
    "    # Type 0 is minority\n",
    "    min_count = count_0\n",
    "    majority_class = type_1\n",
    "    minority_class = type_0\n",
    "    print(\"Type 0 is minority\")\n",
    "else:\n",
    "    # Type 1 is minority\n",
    "    min_count = count_1\n",
    "    majority_class = type_0\n",
    "    minority_class = type_1\n",
    "    print(\"Type 1 is minority\")\n",
    "\n",
    "# Undersample majority class\n",
    "majority_undersampled = majority_class.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine the undersampled majority with minority\n",
    "balanced_df = pd.concat([majority_undersampled, minority_class])\n",
    "\n",
    "# Shuffle the data\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"\\nBalanced distribution:\")\n",
    "print(balanced_df['type'].value_counts())\n",
    "\n",
    "# Save the balanced dataset\n",
    "balanced_df.to_csv('data/995,000_row_balanced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.05859375\n",
      "LogisticRegression accuracy:  0.94140625\n",
      "LogisticRegression F1 score:  0.9620334851037895\n"
     ]
    }
   ],
   "source": [
    "## Test unblananaced data\n",
    "# Label all to either 1 or 0\n",
    "labels = df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=0)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=0)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.07869512502001692\n",
      "LogisticRegression accuracy:  0.921304874979983\n",
      "LogisticRegression F1 score:  0.9223756656737973\n"
     ]
    }
   ],
   "source": [
    "## Test balanced data \n",
    "\n",
    "# Label all to either 1 or 0\n",
    "labels = balanced_df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = balanced_df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=0)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=0)\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliable and politcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_13931/171397139.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['political' 'fake' 'satire' 'reliable' 'conspiracy' 'unreliable' 'bias'\n",
      " 'rumor' 'clickbait' 'hate' 'junksci']\n",
      "[0 1]\n",
      "type\n",
      "1    490597\n",
      "0    413082\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')\n",
    "\n",
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"unreliable\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,  \"clickbait\": 1,\n",
    "             \"reliable\": 0,   \"political\": 0}\n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].unique())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].unique())\n",
    "print(df[\"type\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 0 is minority\n",
      "\n",
      "Balanced distribution:\n",
      "type\n",
      "1    413082\n",
      "0    413082\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Balancing the dataset\n",
    "# Separate the two classes\n",
    "type_0 = df[df['type'] == 0]\n",
    "type_1 = df[df['type'] == 1]\n",
    "\n",
    "# Check class sizes\n",
    "count_0 = len(type_0)\n",
    "count_1 = len(type_1)\n",
    "\n",
    "# Identify minority and majority classes\n",
    "if count_0 < count_1:\n",
    "    # Type 0 is minority\n",
    "    min_count = count_0\n",
    "    majority_class = type_1\n",
    "    minority_class = type_0\n",
    "    print(\"Type 0 is minority\")\n",
    "else:\n",
    "    # Type 1 is minority\n",
    "    min_count = count_1\n",
    "    majority_class = type_0\n",
    "    minority_class = type_1\n",
    "    print(\"Type 1 is minority\")\n",
    "\n",
    "# Undersample majority class\n",
    "majority_undersampled = majority_class.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Combine the undersampled majority with minority\n",
    "balanced_df = pd.concat([majority_undersampled, minority_class])\n",
    "\n",
    "# Shuffle the data\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify the new distribution\n",
    "print(\"\\nBalanced distribution:\")\n",
    "print(balanced_df['type'].value_counts())\n",
    "\n",
    "# Save the balanced dataset\n",
    "balanced_df.to_csv('data/995,000_row_balanced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.13910897662889518\n",
      "LogisticRegression accuracy:  0.8608910233711048\n",
      "LogisticRegression F1 score:  0.8774290422285275\n"
     ]
    }
   ],
   "source": [
    "## Test unblananaced data\n",
    "# Label all to either 1 or 0\n",
    "labels = df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=0)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=0)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = ((y_test-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression MSE:  0.1398380478593994\n",
      "LogisticRegression accuracy:  0.8601619521406006\n",
      "LogisticRegression F1 score:  0.8643647933127487\n"
     ]
    }
   ],
   "source": [
    "## Test balanced data \n",
    "\n",
    "# Label all to either 1 or 0\n",
    "labels = balanced_df[\"type\"]\n",
    "\n",
    "# Using Bag of Words\n",
    "texts = balanced_df[\"content\"]\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=0)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=0)\n",
    "model = LogisticRegression(max_iter=1000,verbose=1)\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = ((y_val-y_pred)**2).mean()\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(\"LogisticRegression MSE: \", mse)\n",
    "print(\"LogisticRegression accuracy: \", acc)\n",
    "print(\"LogisticRegression F1 score: \", f1_score(y_val, y_pred))\n"
   ]
=======
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
