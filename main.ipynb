{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from cleantext.clean import clean\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "import swifter\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/news_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:00<00:00, 554.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to data/news_sample_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text_library(text):\n",
    "    return clean(str(text),  # Convert to string in case of non-string input\n",
    "        fix_unicode=True,               \n",
    "        to_ascii=True,                  \n",
    "        lower=True,                     \n",
    "        no_line_breaks=True,            \n",
    "        lang=\"en\"                       \n",
    "    )\n",
    "df['content'] = df['content'].astype(str).progress_apply(clean_text_library)\n",
    "\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/news_sample_1.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns \n",
    "patterns = {\n",
    "    r\"[\\w]+ [\\d]+, [\\d]+\": \"\",                    # DATE method 1\n",
    "    r\"[\\d]+[\\w]+ [\\w]+ [\\d]+\": \"\",                # DATE method 2\n",
    "    r\"[\\d]+\\/?-?\\.?[\\d]+\\/?-?\\.?[\\d]+\": \"\",       # DATE method 3 (fixed brackets)\n",
    "    r\"[\\w]+ \\d\\d?[\\w]?[\\w]?,? [\\d]{2,4}\": \"\",     # DATE method 4\n",
    "    r\"([\\d]{1,2}[\\w]*) ([\\w]*),? ([\\d]{2,4})\": \"\",# DATE method 5 (fixed range)\n",
    "    r\"\\b(\\d+(st|nd|rd|th|s))\\b\": \"\"                # NUM\n",
    "}\n",
    "\n",
    "def clean_dates(df):\n",
    "    for pattern, replacement in patterns.items():\n",
    "        df['content'] = df['content'].str.replace(pattern, replacement, regex=True)\n",
    "    return df\n",
    "\n",
    "df = clean_dates(df)\n",
    "df.to_csv('data/news_sample_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:00<00:00, 565.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_text_library(text):\n",
    "    return clean(text,\n",
    "    no_urls=True,\n",
    "    lower=False,                   \n",
    "    no_emails=True,               \n",
    "    no_phone_numbers=True,        \n",
    "    no_numbers=True,               \n",
    "    no_digits=True,                \n",
    "    no_currency_symbols=True,      \n",
    "    no_punct=True,                \n",
    "    replace_with_punct=\"\",          \n",
    "    replace_with_url=\"URL\",\n",
    "    replace_with_email=\"EMAIL\",\n",
    "    replace_with_phone_number=\"\",\n",
    "    replace_with_number=\"NUM\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"\",\n",
    "    lang=\"en\"                       \n",
    ")\n",
    "# Cleaning the content column of the dataset\n",
    "df['content'] = df['content'].progress_apply(clean_text_library)\n",
    "df.to_csv('data/news_sample_3.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the content column of the dataset\n",
    "df['content'] = df['content'].apply(word_tokenize)\n",
    "df.to_csv('data/news_sample_4.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def rs(list):\n",
    "    return [word for word in list if word.lower() not in stop_words]\n",
    "\n",
    "df['content'] = df['content'].apply(rs)\n",
    "df.to_csv('data/news_sample_5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def setmming(list):\n",
    "    return [stemmer.stem(word) for word in list]\n",
    "\n",
    "df['content'] = df['content'].apply(setmming)\n",
    "df.to_csv('data/news_sample_6.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the df_5_content text: 16371\n",
      "Number of unique words in the df_6_content text: 10958\n",
      "0.6693543460998106\n"
     ]
    }
   ],
   "source": [
    "# Defines the regex for finding words  \n",
    "word_pattern = re.compile(r\"\\b\\w+\\b\")\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df_5 = pd.read_csv('data/news_sample_5.csv')\n",
    "df_6 = pd.read_csv('data/news_sample_6.csv')\n",
    "# Extracting all words of the \"news_sample.csv\" and the cleaned version\n",
    "df_5_content = \" \".join(df_5[\"content\"].astype(str))\n",
    "df_6_content = \" \".join(df_6[\"content\"].astype(str))\n",
    "\n",
    "df_5_content = word_pattern.findall(df_5_content)\n",
    "df_6_content = word_pattern.findall(df_6_content)\n",
    "\n",
    "# Extracting all unique words of the \"news_sample.csv\" and the cleaned version\n",
    "len_df_5_content = len(set(df_5_content))\n",
    "len_df_6_content = len(set(df_6_content))\n",
    "\n",
    "# Printing the unique words of the raw version, and the unique words after it has been cleaned \n",
    "print(f\"Number of unique words in the df_5_content text: {len_df_5_content}\")\n",
    "print(f\"Number of unique words in the df_6_content text: {len_df_6_content}\")\n",
    "\n",
    "print(len_df_6_content/len_df_5_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_45185/100071585.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_995 = pd.read_csv('data/995,000_rows.csv')\n"
     ]
    }
   ],
   "source": [
    "df_995 = pd.read_csv('data/995,000_rows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [16:53<00:00, 981.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned the newline tabs spaces; data saved to data/995,000_row_cleaned_spaces.csv\n"
     ]
    }
   ],
   "source": [
    "def clean_space(txt):\n",
    "    return clean(str(txt), fix_unicode=True, to_ascii=True, lower=True, no_line_breaks=True,lang=\"en\")\n",
    "\n",
    "# Clean the text remove newline tabs spaces\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(clean_space)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_cleaned_spaces.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned the newline tabs spaces; data saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [04:58<00:00, 3337.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove dates; data saved to data/995,000_row_remove_dates.csv\n"
     ]
    }
   ],
   "source": [
    "patterns = {\n",
    "    r\"[\\w]+ [\\d]+, [\\d]+\": \"\",                     # DATE method 1\n",
    "    r\"[\\d]+[\\w]+ [\\w]+ [\\d]+\": \"\",                 # DATE method 2\n",
    "    r\"[\\d]+\\/?-?\\.?[\\d]+\\/?-?\\.?[\\d]+\": \"\",        # DATE method 3 (fixed brackets)\n",
    "    r\"[\\w]+ \\d\\d?[\\w]?[\\w]?,? [\\d]{2,4}\": \"\",      # DATE method 4\n",
    "    r\"([\\d]{1,2}[\\w]*) ([\\w]*),? ([\\d]{2,4})\": \"\", # DATE method 5 (fixed range)\n",
    "    r\"\\b(\\d+(st|nd|rd|th|s))\\b\": \"\"                # NUM\n",
    "}\n",
    "compiled_patterns = [(re.compile(pattern), replacement) for pattern, replacement in patterns.items()]\n",
    "\n",
    "def remove_dates(txt):\n",
    "    for pattern, replacement in compiled_patterns:\n",
    "        txt = re.sub(pattern, replacement, txt)\n",
    "    return txt\n",
    "\n",
    "# Remove dates\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(remove_dates)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_remove_dates.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Remove dates; data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [18:59<00:00, 873.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed urls, emails, phone numbers, numbers, digits, currency symbols, punctuations; data saved to data/995,000_row_removed_urls.csv\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(txt):\n",
    "    return clean(txt, no_urls=True, no_emails=True, no_phone_numbers=True, no_numbers=True, no_digits=True, no_currency_symbols=True, lower=False, no_punct=True, replace_with_punct=\"\", replace_with_url=\"URL\", replace_with_email=\"EMAIL\", replace_with_phone_number=\"\", replace_with_number=\"NUM\", replace_with_digit=\"0\", replace_with_currency_symbol=\"\", lang=\"en\")\n",
    "\n",
    "# Remove urls, emails, phone numbers, numbers, digits, currency symbols, punctuations\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(remove_urls)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_removed_urls.csv'\n",
    "print(f\"Removed urls, emails, phone numbers, numbers, digits, currency symbols, punctuations; data saved to {output_path}\")\n",
    "df_995.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [09:33<00:00, 1735.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized; data saved to data/995,000_row_tokenized.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the text\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(word_tokenize)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_tokenized.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Tokenized; data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_45185/3962938543.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_995 = pd.read_csv('data/995,000_row_tokenized.csv')\n"
     ]
    }
   ],
   "source": [
    "df_995 = pd.read_csv('data/995,000_row_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [12:53<00:00, 1286.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row_removed_stop_words; data saved to data/995,000_row_removed_stop_words.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words(lst):\n",
    "    return [word for word in lst if word.lower() not in stop_words]\n",
    "\n",
    "# Remove stop words\n",
    "df_995['content'] = df_995['content'].swifter.progress_bar(True).apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_removed_stop_words.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Row_removed_stop_words; data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "0    [plus, one, article, on, google, plus, thanks,...\n",
      "1    [the, cost, of, the, best, senate, banking, co...\n",
      "2    [man, awoken, from, NUMyear, coma, commits, su...\n",
      "3    [when, julia, geist, was, asked, to, draw, a, ...\n",
      "4    [compiled, studies, on, vaccine, dangers, acti...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "print(df_995['content'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:00<00:00, 32636.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   id                domain        type  \\\n",
      "0           0  141               awm.com  unreliable   \n",
      "1           1  256     beforeitsnews.com        fake   \n",
      "2           2  700           cnnnext.com  unreliable   \n",
      "3           3  768               awm.com  unreliable   \n",
      "4           4  791  bipartisanreport.com   clickbait   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://awm.com/church-congregation-brings-gift...   \n",
      "1  http://beforeitsnews.com/awakening-start-here/...   \n",
      "2  http://www.cnnnext.com/video/18526/never-hike-...   \n",
      "3  http://awm.com/elusive-alien-of-the-sea-caught...   \n",
      "4  http://bipartisanreport.com/2018/01/21/trumps-...   \n",
      "\n",
      "                                             content  \\\n",
      "0  [sometimes, power, christmas, make, wild, wond...   \n",
      "1  [awakening, NUM, strands, dna, reconnecting, m...   \n",
      "2  [never, hike, alone, friday, fan, film, usa, |...   \n",
      "3  [rare, shark, caught, scientists, left, blunde...   \n",
      "4  [donald, trump, unnerving, ability, ability, c...   \n",
      "\n",
      "                   scraped_at                 inserted_at  \\\n",
      "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "\n",
      "                   updated_at  \\\n",
      "0  2018-02-02 01:19:41.756664   \n",
      "1  2018-02-02 01:19:41.756664   \n",
      "2  2018-02-02 01:19:41.756664   \n",
      "3  2018-02-02 01:19:41.756664   \n",
      "4  2018-02-02 01:19:41.756664   \n",
      "\n",
      "                                               title          authors  \\\n",
      "0  Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
      "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
      "2  Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
      "3  Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
      "4  Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
      "\n",
      "   keywords meta_keywords                                   meta_description  \\\n",
      "0       NaN          ['']                                                NaN   \n",
      "1       NaN          ['']                                                NaN   \n",
      "2       NaN          ['']  Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
      "3       NaN          ['']                                                NaN   \n",
      "4       NaN          ['']                                                NaN   \n",
      "\n",
      "  tags  summary  \n",
      "0  NaN      NaN  \n",
      "1  NaN      NaN  \n",
      "2  NaN      NaN  \n",
      "3  NaN      NaN  \n",
      "4  NaN      NaN  \n",
      "0    [sometimes, power, christmas, make, wild, wond...\n",
      "1    [awakening, NUM, strands, dna, reconnecting, m...\n",
      "2    [never, hike, alone, friday, fan, film, usa, |...\n",
      "3    [rare, shark, caught, scientists, left, blunde...\n",
      "4    [donald, trump, unnerving, ability, ability, c...\n",
      "Name: content, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Load NLTK stop words as a set for fast lookups\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(word_list):\n",
    "    \"\"\"Remove stop words efficiently from a list of words using NLTK.\"\"\"\n",
    "    return [word for word in word_list if word.lower() not in stop_words_set]\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/news_sample_4.csv')\n",
    "\n",
    "# Convert 'content' column from string to actual list if needed\n",
    "df['content'] = df['content'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x)\n",
    "\n",
    "# Apply stop word removal with swifter for efficiency\n",
    "df['content'] = df['content'].swifter.progress_bar(True).apply(remove_stop_words)\n",
    "\n",
    "# Print results\n",
    "print(df.head())\n",
    "print(df['content'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply:   1%|          | 12413/995000 [00:34<26:06, 627.13it/s]  "
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(lst):\n",
    "    return [stemmer.stem(word) for word in lst]\n",
    "\n",
    "#  Stem the text\n",
    "df_995['content'] = df_995['content'].swifter.progress_bar(True).apply(setmming)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_cleaned.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Fully cleaned; data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
