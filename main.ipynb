{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from cleantext.clean import clean\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/news_sample.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/news_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text_library\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clean(\u001b[38;5;28mstr\u001b[39m(text),  \u001b[38;5;66;03m# Convert to string in case of non-string input\u001b[39;00m\n\u001b[1;32m      3\u001b[0m         fix_unicode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,               \n\u001b[1;32m      4\u001b[0m         to_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,                  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m         lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m                       \n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(clean_text_library)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Save cleaned data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/news_sample_1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_text_library(text):\n",
    "    return clean(str(text),  # Convert to string in case of non-string input\n",
    "        fix_unicode=True,               \n",
    "        to_ascii=True,                  \n",
    "        lower=True,                     \n",
    "        no_line_breaks=True,            \n",
    "        lang=\"en\"                       \n",
    "    )\n",
    "df['content'] = df['content'].astype(str).apply(clean_text_library)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/news_sample_1.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns \n",
    "patterns = {\n",
    "    r\"[\\w]+ [\\d]+, [\\d]+\": \"\",                    # DATE method 1\n",
    "    r\"[\\d]+[\\w]+ [\\w]+ [\\d]+\": \"\",                # DATE method 2\n",
    "    r\"[\\d]+\\/?-?\\.?[\\d]+\\/?-?\\.?[\\d]+\": \"\",       # DATE method 3 (fixed brackets)\n",
    "    r\"[\\w]+ \\d\\d?[\\w]?[\\w]?,? [\\d]{2,4}\": \"\",     # DATE method 4\n",
    "    r\"([\\d]{1,2}[\\w]*) ([\\w]*),? ([\\d]{2,4})\": \"\",# DATE method 5 (fixed range)\n",
    "    r\"\\b(\\d+(st|nd|rd|th|s))\\b\": \"\"                # NUM\n",
    "}\n",
    "\n",
    "def clean_dates(df):\n",
    "    for pattern, replacement in patterns.items():\n",
    "        df['content'] = df['content'].str.replace(pattern, replacement, regex=True)\n",
    "    return df\n",
    "\n",
    "df = clean_dates(df)\n",
    "df.to_csv('data/news_sample_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_library(text):\n",
    "    return clean(text,\n",
    "    no_urls=True,                  \n",
    "    no_emails=True,               \n",
    "    no_phone_numbers=True,        \n",
    "    no_numbers=True,               \n",
    "    no_digits=True,                \n",
    "    no_currency_symbols=True,      \n",
    "    no_punct=True,                \n",
    "    replace_with_punct=\"\",          \n",
    "    replace_with_url=\"\",\n",
    "    replace_with_email=\"\",\n",
    "    replace_with_phone_number=\"\",\n",
    "    replace_with_number=\"\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"\",\n",
    "    lang=\"en\"                       \n",
    ")\n",
    "# Cleaning the content column of the dataset\n",
    "df['content'] = df['content'].apply(clean_text_library)\n",
    "df.to_csv('data/news_sample_3.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the content column of the dataset\n",
    "df['content'] = df['content'].apply(word_tokenize)\n",
    "df.to_csv('data/news_sample_4.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def rs(list):\n",
    "    return [word for word in list if word.lower() not in stop_words]\n",
    "\n",
    "df['content'] = df['content'].apply(rs)\n",
    "df.to_csv('data/news_sample_5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def setmming(list):\n",
    "    return [stemmer.stem(word) for word in list]\n",
    "\n",
    "df['content'] = df['content'].apply(setmming)\n",
    "df.to_csv('data/news_sample_6.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the df_5_content text: 16371\n",
      "Number of unique words in the df_6_content text: 10958\n",
      "0.6693543460998106\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Defines the regex for finding words  \n",
    "word_pattern = re.compile(r\"\\b\\w+\\b\")\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df_5 = pd.read_csv('data/news_sample_5.csv')\n",
    "df_6 = pd.read_csv('data/news_sample_6.csv')\n",
    "# Extracting all words of the \"news_sample.csv\" and the cleaned version\n",
    "df_5_content = \" \".join(df_5[\"content\"].astype(str))\n",
    "df_6_content = \" \".join(df_6[\"content\"].astype(str))\n",
    "\n",
    "df_5_content = word_pattern.findall(df_5_content)\n",
    "df_6_content = word_pattern.findall(df_6_content)\n",
    "\n",
    "# Extracting all unique words of the \"news_sample.csv\" and the cleaned version\n",
    "len_df_5_content = len(set(df_5_content))\n",
    "len_df_6_content = len(set(df_6_content))\n",
    "\n",
    "# Printing the unique words of the raw version, and the unique words after it has been cleaned \n",
    "print(f\"Number of unique words in the df_5_content text: {len_df_5_content}\")\n",
    "print(f\"Number of unique words in the df_6_content text: {len_df_6_content}\")\n",
    "\n",
    "print(len_df_6_content/len_df_5_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
