{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "/opt/miniconda3/envs/GDSvenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from cleantext.clean import clean\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "import swifter\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/news_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:00<00:00, 554.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to data/news_sample_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text_library(text):\n",
    "    return clean(str(text),  # Convert to string in case of non-string input\n",
    "        fix_unicode=True,               \n",
    "        to_ascii=True,                  \n",
    "        lower=True,                     \n",
    "        no_line_breaks=True,            \n",
    "        lang=\"en\"                       \n",
    "    )\n",
    "df['content'] = df['content'].astype(str).progress_apply(clean_text_library)\n",
    "\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/news_sample_1.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns \n",
    "patterns = {\n",
    "    r\"[\\w]+ [\\d]+, [\\d]+\": \"\",                    # DATE method 1\n",
    "    r\"[\\d]+[\\w]+ [\\w]+ [\\d]+\": \"\",                # DATE method 2\n",
    "    r\"[\\d]+\\/?-?\\.?[\\d]+\\/?-?\\.?[\\d]+\": \"\",       # DATE method 3 (fixed brackets)\n",
    "    r\"[\\w]+ \\d\\d?[\\w]?[\\w]?,? [\\d]{2,4}\": \"\",     # DATE method 4\n",
    "    r\"([\\d]{1,2}[\\w]*) ([\\w]*),? ([\\d]{2,4})\": \"\",# DATE method 5 (fixed range)\n",
    "    r\"\\b(\\d+(st|nd|rd|th|s))\\b\": \"\"                # NUM\n",
    "}\n",
    "\n",
    "def clean_dates(df):\n",
    "    for pattern, replacement in patterns.items():\n",
    "        df['content'] = df['content'].str.replace(pattern, replacement, regex=True)\n",
    "    return df\n",
    "\n",
    "df = clean_dates(df)\n",
    "df.to_csv('data/news_sample_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:00<00:00, 565.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_text_library(text):\n",
    "    return clean(text,\n",
    "    no_urls=True,\n",
    "    lower=False,                   \n",
    "    no_emails=True,               \n",
    "    no_phone_numbers=True,        \n",
    "    no_numbers=True,               \n",
    "    no_digits=True,                \n",
    "    no_currency_symbols=True,      \n",
    "    no_punct=True,                \n",
    "    replace_with_punct=\"\",          \n",
    "    replace_with_url=\"URL\",\n",
    "    replace_with_email=\"EMAIL\",\n",
    "    replace_with_phone_number=\"\",\n",
    "    replace_with_number=\"NUM\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"\",\n",
    "    lang=\"en\"                       \n",
    ")\n",
    "# Cleaning the content column of the dataset\n",
    "df['content'] = df['content'].progress_apply(clean_text_library)\n",
    "df.to_csv('data/news_sample_3.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the content column of the dataset\n",
    "df['content'] = df['content'].apply(word_tokenize)\n",
    "df.to_csv('data/news_sample_4.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def rs(list):\n",
    "    return [word for word in list if word.lower() not in stop_words]\n",
    "\n",
    "df['content'] = df['content'].apply(rs)\n",
    "df.to_csv('data/news_sample_5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def setmming(list):\n",
    "    return [stemmer.stem(word) for word in list]\n",
    "\n",
    "df['content'] = df['content'].apply(setmming)\n",
    "df.to_csv('data/news_sample_6.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the df_5_content text: 16371\n",
      "Number of unique words in the df_6_content text: 10958\n",
      "0.6693543460998106\n"
     ]
    }
   ],
   "source": [
    "# Defines the regex for finding words  \n",
    "word_pattern = re.compile(r\"\\b\\w+\\b\")\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df_5 = pd.read_csv('data/news_sample_5.csv')\n",
    "df_6 = pd.read_csv('data/news_sample_6.csv')\n",
    "# Extracting all words of the \"news_sample.csv\" and the cleaned version\n",
    "df_5_content = \" \".join(df_5[\"content\"].astype(str))\n",
    "df_6_content = \" \".join(df_6[\"content\"].astype(str))\n",
    "\n",
    "df_5_content = word_pattern.findall(df_5_content)\n",
    "df_6_content = word_pattern.findall(df_6_content)\n",
    "\n",
    "# Extracting all unique words of the \"news_sample.csv\" and the cleaned version\n",
    "len_df_5_content = len(set(df_5_content))\n",
    "len_df_6_content = len(set(df_6_content))\n",
    "\n",
    "# Printing the unique words of the raw version, and the unique words after it has been cleaned \n",
    "print(f\"Number of unique words in the df_5_content text: {len_df_5_content}\")\n",
    "print(f\"Number of unique words in the df_6_content text: {len_df_6_content}\")\n",
    "\n",
    "print(len_df_6_content/len_df_5_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_45185/100071585.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_995 = pd.read_csv('data/995,000_rows.csv')\n"
     ]
    }
   ],
   "source": [
    "df_995 = pd.read_csv('data/995,000_rows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [16:53<00:00, 981.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned the newline tabs spaces; data saved to data/995,000_row_cleaned_spaces.csv\n"
     ]
    }
   ],
   "source": [
    "def clean_space(txt):\n",
    "    return clean(str(txt), fix_unicode=True, to_ascii=True, lower=True, no_line_breaks=True,lang=\"en\")\n",
    "\n",
    "# Clean the text remove newline tabs spaces\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(clean_space)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_cleaned_spaces.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned the newline tabs spaces; data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_995 = pd.read_csv('data/995,000_row_cleaned_spaces.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [04:58<00:00, 3337.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove dates; data saved to data/995,000_row_remove_dates.csv\n"
     ]
    }
   ],
   "source": [
    "patterns = {\n",
    "    r\"[\\w]+ [\\d]+, [\\d]+\": \"\",                     # DATE method 1\n",
    "    r\"[\\d]+[\\w]+ [\\w]+ [\\d]+\": \"\",                 # DATE method 2\n",
    "    r\"[\\d]+\\/?-?\\.?[\\d]+\\/?-?\\.?[\\d]+\": \"\",        # DATE method 3 (fixed brackets)\n",
    "    r\"[\\w]+ \\d\\d?[\\w]?[\\w]?,? [\\d]{2,4}\": \"\",      # DATE method 4\n",
    "    r\"([\\d]{1,2}[\\w]*) ([\\w]*),? ([\\d]{2,4})\": \"\", # DATE method 5 (fixed range)\n",
    "    r\"\\b(\\d+(st|nd|rd|th|s))\\b\": \"\"                # NUM\n",
    "}\n",
    "compiled_patterns = [(re.compile(pattern), replacement) for pattern, replacement in patterns.items()]\n",
    "\n",
    "def remove_dates(txt):\n",
    "    for pattern, replacement in compiled_patterns:\n",
    "        txt = re.sub(pattern, replacement, txt)\n",
    "    return txt\n",
    "\n",
    "# Remove dates\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(remove_dates)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_remove_dates.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Remove dates; data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_995 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/995,000_row_remove_dates.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_995 = pd.read_csv('data/995,000_row_remove_dates.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [18:59<00:00, 873.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed urls, emails, phone numbers, numbers, digits, currency symbols, punctuations; data saved to data/995,000_row_removed_urls.csv\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(txt):\n",
    "    return clean(txt, no_urls=True, no_emails=True, no_phone_numbers=True, no_numbers=True, no_digits=True, no_currency_symbols=True, lower=False, no_punct=True, replace_with_punct=\"\", replace_with_url=\"URL\", replace_with_email=\"EMAIL\", replace_with_phone_number=\"\", replace_with_number=\"NUM\", replace_with_digit=\"0\", replace_with_currency_symbol=\"\", lang=\"en\")\n",
    "\n",
    "# Remove urls, emails, phone numbers, numbers, digits, currency symbols, punctuations\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(remove_urls)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_removed_urls.csv'\n",
    "print(f\"Removed urls, emails, phone numbers, numbers, digits, currency symbols, punctuations; data saved to {output_path}\")\n",
    "df_995.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_3386/482267536.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_995 = pd.read_csv('data/995,000_row_removed_urls.csv')\n"
     ]
    }
   ],
   "source": [
    "df_995 = pd.read_csv('data/995,000_row_removed_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [09:33<00:00, 1735.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized; data saved to data/995,000_row_tokenized.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the text\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(word_tokenize)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_tokenized.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Tokenized; data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_3672/3962938543.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_995 = pd.read_csv('data/995,000_row_tokenized.csv')\n"
     ]
    }
   ],
   "source": [
    "df_995 = pd.read_csv('data/995,000_row_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [11:18<00:00, 1467.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row_removed_stop_words; data saved to data/995,000_row_removed_stop_words.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words(lst):\n",
    "    return [word for word in ast.literal_eval(lst) if word.lower() not in stop_words]\n",
    "\n",
    "# Remove stop words\n",
    "df_995['content'] = df_995['content'].swifter.progress_bar(True).apply(remove_stop_words)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_removed_stop_words.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Row_removed_stop_words; data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_3672/2746261549.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_995 = pd.read_csv('data/995,000_row_removed_stop_words.csv')\n"
     ]
    }
   ],
   "source": [
    "df_995 = pd.read_csv('data/995,000_row_removed_stop_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 995000/995000 [27:18<00:00, 607.21it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully cleaned; data saved to data/995,000_row_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(lst):\n",
    "    return [stemmer.stem(word) for word in ast.literal_eval(lst)]\n",
    "\n",
    "#  Stem the text\n",
    "df_995['content'] = df_995['content'].swifter.progress_bar(True).apply(stemming)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_cleaned.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Fully cleaned; data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
