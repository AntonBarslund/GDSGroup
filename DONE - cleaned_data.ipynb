{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Fake News Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from cleantext.clean import clean\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "import swifter\n",
    "import ast\n",
    "\n",
    "# initialize tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "df_995 = pd.read_csv('data/995,000_rows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:00<00:00, 297.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned the newline tabs spaces; data saved to data/995,000_row_cleaned_spaces.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Functoin to clean text using clean-text library\n",
    "# This function will remove non-ascii characters, convert to lowercase, \n",
    "# remove line breaks, and remove non-english text\n",
    "def clean_space(txt):\n",
    "    return clean(str(txt), \n",
    "                 fix_unicode=True, \n",
    "                 to_ascii=True, \n",
    "                 lower=True, \n",
    "                 no_line_breaks=True,\n",
    "                 lang=\"en\")\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(clean_space)\n",
    "\n",
    "# Save the new version of cleaned data to a new csv file\n",
    "output_path = 'data/995,000_row_cleaned_spaces.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned the newline tabs spaces; data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the new version of the Fake news dataset\n",
    "df_995 = pd.read_csv('data/995,000_row_cleaned_spaces.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:00<00:00, 1136.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove dates; data saved to data/995,000_row_remove_dates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of regex patterns to remove various date formats and numerical suffixes from text\n",
    "# Each pattern is matched and replaced with an empty string to clean the text\n",
    "patterns = {\n",
    "    r\"[\\w]+ [\\d]+, [\\d]+\": \"\",                     # DATE method 1: \"January 15, 2023\"\n",
    "    r\"[\\d]+[\\w]+ [\\w]+ [\\d]+\": \"\",                 # DATE method 2: \"15th January 2023\" \n",
    "    r\"[\\d]+\\/?-?\\.?[\\d]+\\/?-?\\.?[\\d]+\": \"\",        # DATE method 3: \"01/15/2023\", \"01-15-2023\", \"01.15.2023\"\n",
    "    r\"[\\w]+ \\d\\d?[\\w]?[\\w]?,? [\\d]{2,4}\": \"\",      # DATE method 4: \"January 15th, 2023\", \"Jan 1, 2023\"\n",
    "    r\"([\\d]{1,2}[\\w]*) ([\\w]*),? ([\\d]{2,4})\": \"\", # DATE method 5: \"15 January, 2023\", capturing day, month, year\n",
    "    r\"\\b(\\d+(st|nd|rd|th|s))\\b\": \"\"                # NUM: Ordinal numbers like \"1st\", \"2nd\", \"3rd\", \"4th\" or plural suffixes\n",
    "}\n",
    "# Creates a list of tuples containing compiled_regex and replacement_string\n",
    "compiled_patterns = [(re.compile(pattern), replacement) for pattern, replacement in patterns.items()]\n",
    "\n",
    "# Function that remove all date patterns from the text \n",
    "def remove_dates(txt):\n",
    "    for pattern, replacement in compiled_patterns:\n",
    "        txt = re.sub(pattern, replacement, txt)\n",
    "    return txt\n",
    "\n",
    "# Apply the date removal function to each row in the content column\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(remove_dates)\n",
    "\n",
    "# Save the new version of cleaned data to a new csv file\n",
    "output_path = 'data/995,000_row_remove_dates.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Remove dates; data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the new version of the Fake news dataset\n",
    "df_995 = pd.read_csv('data/995,000_row_remove_dates.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:00<00:00, 273.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed urls, emails, phone numbers, numbers, digits, currency symbols, punctuations; data saved to data/995,000_row_removed_urls.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(txt):\n",
    "    return clean(txt, \n",
    "        no_urls=True,                    # Remove all URLs from text\n",
    "        no_emails=True,                  # Remove email addresses\n",
    "        no_phone_numbers=True,           # Remove phone number patterns\n",
    "        no_numbers=True,                 # Remove standalone numbers\n",
    "        no_digits=True,                  # Remove individual digits\n",
    "        no_currency_symbols=True,        # Remove currency symbols like $, €, etc.\n",
    "        lower=False,                     # Preserve original case \n",
    "        no_punct=True,                   # Remove all punctuation\n",
    "        replace_with_punct=\"\",           # Replace punctuation with empty string\n",
    "        replace_with_url=\"URL\",          # Replace URLs with the token \"URL\"\n",
    "        replace_with_email=\"EMAIL\",      # Replace emails with the token \"EMAIL\"\n",
    "        replace_with_phone_number=\"\",    # Replace phone numbers with empty string\n",
    "        replace_with_number=\"NUM\",       # Replace numbers with the token \"NUM\"\n",
    "        replace_with_digit=\"0\",          # Replace digits with \"0\"\n",
    "        replace_with_currency_symbol=\"\", # Replace currency symbols with empty string\n",
    "        lang=\"en\"                        # Process as English language text\n",
    "    )\n",
    "\n",
    "# Apply text cleaning to remove specific elements from each document\n",
    "# Using swifter to parallelize processing for better performance\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(remove_urls)\n",
    "\n",
    "# Save the processed data to a new CSV file\n",
    "output_path = 'data/995,000_row_removed_urls.csv'\n",
    "print(f\"Removed urls, emails, phone numbers, numbers, digits, currency symbols, punctuations; data saved to {output_path}\")\n",
    "df_995.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the new version of the Fake news dataset\n",
    "df_995 = pd.read_csv('data/995,000_row_removed_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:00<00:00, 652.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized; data saved to data/995,000_row_tokenized.csv\n"
     ]
    }
   ],
   "source": [
    "# Break text content into individual tokens (words) using NLTK's word_tokenize function\n",
    "# - First converts all content to string type \n",
    "# - Uses swifter to parallelize processing with a visible progress bar\n",
    "# - Applies NLTK's word_tokenize which converts strings into lists of word tokens\n",
    "df_995['content'] = df_995['content'].astype(str).swifter.progress_bar(True).apply(word_tokenize)\n",
    "\n",
    "# Save the tokenized data to a CSV file\n",
    "output_path = 'data/995,000_row_tokenized.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Tokenized; data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the new version of the Fake news dataset\n",
    "df_995 = pd.read_csv('data/995,000_row_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:00<00:00, 737.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row_removed_stop_words; data saved to data/995,000_row_removed_stop_words.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a set of English stop words using NLTK's stopwords corpus\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to filter out stop words from a list of tokens\n",
    "def remove_stop_words(lst):\n",
    "    # ast.literal_eval converts the string representation of a list back to an actual list\n",
    "    # The list comprehension keeps only words that are not in the stop_words set\n",
    "    return [word for word in ast.literal_eval(lst) if word.lower() not in stop_words]\n",
    "\n",
    "# Remove stop words from each article in the Fake news dataset\n",
    "df_995['content'] = df_995['content'].swifter.progress_bar(True).apply(remove_stop_words)\n",
    "\n",
    "# Save the processed data with stop words removed to a CSV file\n",
    "output_path = 'data/995,000_row_removed_stop_words.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Row_removed_stop_words; data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the new version of the Fake news dataset\n",
    "df_995 = pd.read_csv('data/995,000_row_removed_stop_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 250/250 [00:01<00:00, 185.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully cleaned; data saved to data/995,000_row_cleaned1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Porter Stemming algorithm which reduces words to their root/base form\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(lst):\n",
    "    # ast.literal_eval safely converts the string representation of a list back to an actual list\n",
    "    # Each word is reduced to its stem using the Porter algorithm\n",
    "    return [stemmer.stem(word) for word in ast.literal_eval(lst)]\n",
    "\n",
    "# Apply stemming to each article in the dataset\n",
    "df_995['content'] = df_995['content'].swifter.progress_bar(True).apply(stemming)\n",
    "\n",
    "# Save the fully processed and cleaned data to a CSV file\n",
    "output_path = 'data/995,000_row_cleaned.csv'\n",
    "df_995.to_csv(output_path, index=False)\n",
    "print(f\"Fully cleaned; data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
