{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "import swifter\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "# Required resources; download once\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/995,000_row_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "# Function to create a vocabulary with word counts\n",
    "def create_vocab(lst):\n",
    "    # Convert the string representation of a list into a actual list\n",
    "    lst = ast.literal_eval(lst)\n",
    "    # Iterate over words in the list and update their counts in the dictionary\n",
    "    for word in lst:\n",
    "        if word in vocab:\n",
    "            vocab[word] += 1 # Increment count if the word already exists\n",
    "        else:\n",
    "            vocab[word] = 1 # Add the word to the dictionary with an initial count of 1\n",
    "\n",
    "# Apply the function to the 'content' column\n",
    "df['content'].swifter.progress_bar(True).apply(create_vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortend_vocab = dict(sorted(vocab.items(), key=lambda item: item[1], reverse=True)[:10000])\n",
    "with open('10000vocab.json', 'w') as f:\n",
    "    json.dump(shortend_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count word frequencies based on the vocabulary\n",
    "def count_freq(lst):\n",
    "    dic = {key: 0 for key in list(vocab)}\n",
    "    # Convert the string representation of a list into a actual list\n",
    "    lst = ast.literal_eval(lst)\n",
    "    # Iterate over words in the list and update their counts in the dictionary\n",
    "    for word in lst:\n",
    "        if word in dic:\n",
    "            dic[word] += 1\n",
    "    return [value for value in dic.values()]\n",
    "\n",
    "# Apply the function to the 'content' column\n",
    "df['content'] = df['content'].swifter.progress_bar(True).apply(count_freq)\n",
    "\n",
    "# Save cleaned data\n",
    "output_path = 'data/995,000_row_counted.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Fully cleaned; data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"unreliable\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,\n",
    "             \"reliable\": 0,  \"clickbait\": 0,   \"political\": 0}\n",
    "labels = np.array([label_map.get(row['type']) for _,row in df.iterrows()])\n",
    "features = np.array([count_freq(row['content']) for _,row in df.iterrows()])\n",
    "dataset = np.column_stack((labels, features))\n",
    "\n",
    "# Print shapes\n",
    "print(\"Features shape:\", features.shape)  # (num_samples, num_features)\n",
    "print(\"Labels shape:\", labels.shape)  # (num_samples,)\n",
    "\n",
    "# Example output\n",
    "print(\"First sample:\", dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the CSV file\n",
    "print(\"Starting to read the CSV file...\")\n",
    "df = pd.read_csv('data/995,000_rows.csv')\n",
    "print(\"Successfully read the CSV file!\")\n",
    "\n",
    "# Step 2: Filter the DataFrame\n",
    "print(\"Filtering the DataFrame to include only rows where the 'type' column is 'fake' or 'reliable'...\")\n",
    "filtered_df = df[df['type'].isin(['fake'])]\n",
    "print(\"Filtering complete! The filtered DataFrame contains\", len(filtered_df), \"rows.\")\n",
    "\n",
    "# Step 3: Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv('fake_data.csv', index=False)\n",
    "print(\"Filtered data saved to 'filtered_data.csv'!\")\n",
    "\n",
    "# Final Step: Notify progress completion\n",
    "print(\"All steps completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the processed fake dataset and look into the parts of speech within this subset, creating a new dataset with the analyzed result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fake_data.csv')\n",
    "\n",
    "column_index = 5\n",
    "if df.shape[1] > column_index:\n",
    "    text_column = df.iloc[:, column_index].dropna().astype(str)  # Convert to string\n",
    "    print(f\"Targeted column name: {df.columns[column_index]}\")  # Print column name\n",
    "else:\n",
    "    raise IndexError(\"something's wrong!\")\n",
    "\n",
    "\n",
    "# POS Mapping Dictionary\n",
    "pos_mapping = {\n",
    "    \"CC\": \"Conjunction (coordinating)\", \"CD\": \"Numeral (cardinal)\", \"DT\": \"Determiner\",\n",
    "    \"EX\": \"Existential 'there'\", \"IN\": \"Preposition or subordinating conjunction\",\n",
    "    \"JJ\": \"Adjective\", \"JJR\": \"Adjective (comparative)\", \"JJS\": \"Adjective (superlative)\",\n",
    "    \"LS\": \"List item marker\", \"MD\": \"Modal auxiliary\", \"NN\": \"Noun (singular/mass)\",\n",
    "    \"NNS\": \"Noun (plural)\", \"NNP\": \"Proper Noun (singular)\", \"NNPS\": \"Proper Noun (plural)\",\n",
    "    \"PDT\": \"Pre-determiner\", \"POS\": \"Genitive marker ('s)\", \"PRP\": \"Pronoun (personal)\",\n",
    "    \"PRP$\": \"Pronoun (possessive)\", \"RB\": \"Adverb\", \"RBR\": \"Adverb (comparative)\",\n",
    "    \"RBS\": \"Adverb (superlative)\", \"RP\": \"Particle\", \"TO\": \"To (preposition/infinitive marker)\",\n",
    "    \"UH\": \"Interjection\", \"VB\": \"Verb (base form)\", \"VBD\": \"Verb (past tense)\",\n",
    "    \"VBG\": \"Verb (present participle/gerund)\", \"VBN\": \"Verb (past participle)\",\n",
    "    \"VBP\": \"Verb (present, non-3rd person singular)\", \"VBZ\": \"Verb (present, 3rd person singular)\",\n",
    "    \"WDT\": \"WH-determiner\", \"WP\": \"WH-pronoun\", \"WRB\": \"WH-adverb\"\n",
    "}\n",
    "\n",
    "\n",
    "# Function to process each row\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Convert POS tags to human-readable format\n",
    "    readable_tags = [pos_mapping.get(tag, \"Other\") for _, tag in pos_tags]\n",
    "    \n",
    "    # Count occurrences of each word class\n",
    "    pos_counts = {pos: readable_tags.count(pos) for pos in set(readable_tags)}\n",
    "    \n",
    "    return pos_counts\n",
    "\n",
    "# Apply function to each row and create a new DataFrame\n",
    "df_pos = text_column.apply(process_text).apply(pd.Series).fillna(0).astype(int)\n",
    "\n",
    "# Add original text for reference\n",
    "df_pos.insert(0, \"Original_Text\", text_column)\n",
    "\n",
    "# Display result\n",
    "#print(df_pos.head())\n",
    "\n",
    "# Save to CSV\n",
    "df_pos.to_csv(\"pos_analysis_fake.csv\", index=False)\n",
    "\n",
    "# Optional: Aggregate and plot total POS counts\n",
    "pos_totals = df_pos.drop(columns=[\"Original_Text\"]).sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(pos_totals.index, pos_totals.values)\n",
    "plt.xlabel(\"Part of Speech\")\n",
    "plt.ylabel(\"Total Frequency\")\n",
    "plt.title(\"Total POS Frequency Distribution\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the CSV file\n",
    "print(\"Starting to read the CSV file...\")\n",
    "df = pd.read_csv('data/995,000_rows.csv')\n",
    "print(\"Successfully read the CSV file!\")\n",
    "\n",
    "# Step 2: Filter the DataFrame\n",
    "print(\"Filtering the DataFrame to include only rows where the 'type' column is 'fake' or 'reliable'...\")\n",
    "filtered_df = df[df['type'].isin(['reliable'])]\n",
    "print(\"Filtering complete! The filtered DataFrame contains\", len(filtered_df), \"rows.\")\n",
    "\n",
    "# Step 3: Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv('reliable_data.csv', index=False)\n",
    "print(\"Filtered data saved to 'filtered_data.csv'!\")\n",
    "\n",
    "# Final Step: Notify progress completion\n",
    "print(\"All steps completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the processed reliable dataset and look into the parts of speech within this subset, creating a new dataset with the analyzed result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reliable_data.csv')\n",
    "\n",
    "column_index = 5\n",
    "if df.shape[1] > column_index:\n",
    "    text_column = df.iloc[:, column_index].dropna().astype(str)  # Convert to string\n",
    "    print(f\"Targeted column name: {df.columns[column_index]}\")  # Print column name\n",
    "else:\n",
    "    raise IndexError(\"something's wrong!\")\n",
    "\n",
    "\n",
    "# POS Mapping Dictionary\n",
    "pos_mapping = {\n",
    "    \"CC\": \"Conjunction (coordinating)\", \"CD\": \"Numeral (cardinal)\", \"DT\": \"Determiner\",\n",
    "    \"EX\": \"Existential 'there'\", \"IN\": \"Preposition or subordinating conjunction\",\n",
    "    \"JJ\": \"Adjective\", \"JJR\": \"Adjective (comparative)\", \"JJS\": \"Adjective (superlative)\",\n",
    "    \"LS\": \"List item marker\", \"MD\": \"Modal auxiliary\", \"NN\": \"Noun (singular/mass)\",\n",
    "    \"NNS\": \"Noun (plural)\", \"NNP\": \"Proper Noun (singular)\", \"NNPS\": \"Proper Noun (plural)\",\n",
    "    \"PDT\": \"Pre-determiner\", \"POS\": \"Genitive marker ('s)\", \"PRP\": \"Pronoun (personal)\",\n",
    "    \"PRP$\": \"Pronoun (possessive)\", \"RB\": \"Adverb\", \"RBR\": \"Adverb (comparative)\",\n",
    "    \"RBS\": \"Adverb (superlative)\", \"RP\": \"Particle\", \"TO\": \"To (preposition/infinitive marker)\",\n",
    "    \"UH\": \"Interjection\", \"VB\": \"Verb (base form)\", \"VBD\": \"Verb (past tense)\",\n",
    "    \"VBG\": \"Verb (present participle/gerund)\", \"VBN\": \"Verb (past participle)\",\n",
    "    \"VBP\": \"Verb (present, non-3rd person singular)\", \"VBZ\": \"Verb (present, 3rd person singular)\",\n",
    "    \"WDT\": \"WH-determiner\", \"WP\": \"WH-pronoun\", \"WRB\": \"WH-adverb\"\n",
    "}\n",
    "\n",
    "\n",
    "# Function to process each row\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Convert POS tags to human-readable format\n",
    "    readable_tags = [pos_mapping.get(tag, \"Other\") for _, tag in pos_tags]\n",
    "    \n",
    "    # Count occurrences of each word class\n",
    "    pos_counts = {pos: readable_tags.count(pos) for pos in set(readable_tags)}\n",
    "    \n",
    "    return pos_counts\n",
    "\n",
    "# Apply function to each row and create a new DataFrame\n",
    "df_pos = text_column.apply(process_text).apply(pd.Series).fillna(0).astype(int)\n",
    "\n",
    "# Add original text for reference\n",
    "df_pos.insert(0, \"Original_Text\", text_column)\n",
    "\n",
    "# Display result\n",
    "#print(df_pos.head())\n",
    "\n",
    "# Save to CSV\n",
    "df_pos.to_csv(\"pos_analysis_reliable.csv\", index=False)\n",
    "\n",
    "# Optional: Aggregate and plot total POS counts\n",
    "pos_totals = df_pos.drop(columns=[\"Original_Text\"]).sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(pos_totals.index, pos_totals.values)\n",
    "plt.xlabel(\"Part of Speech\")\n",
    "plt.ylabel(\"Total Frequency\")\n",
    "plt.title(\"Total POS Frequency Distribution\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the relationship between the total amount of words and the total amount of unique words.\n",
    "#### Finding the ratio of unique words per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total amount of word in content\n",
    "total_words = df[\"content\"].swifter.progress_bar(True).apply(lambda x: len(str(x).split())).sum()\n",
    "print(\"Total words in dataset:\", total_words)\n",
    "\n",
    "# Function to extract unique words from a text\n",
    "def extract_unique_words(text):\n",
    "    return set(str(text).split())\n",
    "\n",
    "# Apply function efficiently using swifter\n",
    "unique_word_sets = df[\"content\"].swifter.progress_bar(True).apply(extract_unique_words)\n",
    "\n",
    "# Combine all sets and count unique words\n",
    "total_unique_words = len(set().union(*tqdm(unique_word_sets)))\n",
    "\n",
    "print(\"Total unique words in dataset:\", total_unique_words)\n",
    "\n",
    "# Calculate the percentage of unique words per word\n",
    "print(\"Unique words per word:\", (total_unique_words / total_words) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
