{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# initialize tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a confusion matrix \n",
    "def conf_matrix(y_test, pred_test):    \n",
    "    \n",
    "    # Creating a confusion matrix\n",
    "    con_mat = confusion_matrix(y_test, pred_test)\n",
    "    con_mat = pd.DataFrame(con_mat, range(2), range(2))\n",
    "   \n",
    "    #Ploting the confusion matrix\n",
    "    plt.figure(figsize=(6,6))\n",
    "\n",
    "    plt.title('Confusion Matrix')\n",
    "    sns.set(font_scale=1.5) \n",
    "    sns.heatmap(con_mat, annot=True, annot_kws={\"size\": 16}, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Actual Values')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/hf1tn63x7tl1qjxy1md9xy4w0000gn/T/ipykernel_1718/4153333985.py:2: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/995,000_row_cleaned.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "reliable      218564\n",
      "political     194518\n",
      "bias          133232\n",
      "fake          104883\n",
      "conspiracy     97314\n",
      "rumor          56445\n",
      "clickbait      27412\n",
      "junksci        14040\n",
      "satire         13160\n",
      "hate            8779\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "1    649783\n",
      "0    218564\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Fit and transform the vectorizer on the training data\u001b[39;00m\n\u001b[1;32m     33\u001b[0m X_train \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train)  \n\u001b[0;32m---> 34\u001b[0m X_val \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m       \n\u001b[1;32m     35\u001b[0m X_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test)  \n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Save the fitted vectorizer\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/GDSvenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2128\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \n\u001b[1;32m   2113\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2128\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/GDSvenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1421\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1421\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1423\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/GDSvenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/GDSvenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     64\u001b[0m         doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_analyze\u001b[39m(\n\u001b[1;32m     69\u001b[0m     doc,\n\u001b[1;32m     70\u001b[0m     analyzer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m     ngrams\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m     preprocessor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     74\u001b[0m     decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m     stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m ):\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    a single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m        A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/995,000_row_cleaned.csv')\n",
    "\n",
    "# Remove rows with ivalid values\n",
    "label_map = {\"fake\": 1, \"satire\":1, \"conspiracy\": 1, \"bias\": 1, \"rumor\": 1, \"junksci\": 1, \"hate\": 1,  \"clickbait\": 1,   \"political\": 1,\n",
    "             \"reliable\": 0} # unreliable isn't kept because it's not conclusive data \n",
    "df = df[df[\"type\"].isin(label_map.keys())]  # Keep only rows with valid labels\n",
    "print(df[\"type\"].value_counts())\n",
    "df[\"type\"] = df[\"type\"].map(label_map)\n",
    "print(df[\"type\"].value_counts())\n",
    "\n",
    "# Split data \n",
    "texts = df[\"content\"]\n",
    "y = df[\"type\"]\n",
    "\n",
    "# Create a instances of the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # make sure the number of features is 10000\n",
    "    ngram_range=(1, 2),  # Use uni- and bi-grams\n",
    ")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, y, \n",
    "    test_size=0.2,\n",
    "    shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5,\n",
    "    shuffle=False)\n",
    "\n",
    "# Fit and transform the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)  \n",
    "X_val = vectorizer.transform(X_val)       \n",
    "X_test = vectorizer.transform(X_test)  \n",
    "\n",
    "# Save the fitted vectorizer\n",
    "dump(vectorizer, 'models/tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (694677, 10000) \n",
      "y_train.shape: (694677,) \n",
      "\n",
      "X_val.shape: (86835, 10000) \n",
      "y_val.shape: (86835,)\n",
      "\n",
      "X_test.shape: (86835, 10000) \n",
      "y_test.shape: (86835,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the shape of the data\n",
    "print(f\"X_train.shape: {X_train.shape} \\ny_train.shape: {y_train.shape} \\n\")\n",
    "print(f\"X_val.shape: {X_val.shape} \\ny_val.shape: {y_val.shape}\\n\")\n",
    "print(f\"X_test.shape: {X_test.shape} \\ny_test.shape: {y_test.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 75ms/step - f1_score: 0.8627 - loss: 0.2593 - precision_1: 0.9552 - recall_1: 0.8980 - weighted_f1_score: 0.6744 - val_f1_score: 0.8730 - val_loss: 0.1828 - val_precision_1: 0.9619 - val_recall_1: 0.9404 - val_weighted_f1_score: 0.8730\n",
      "Epoch 2/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 76ms/step - f1_score: 0.8984 - loss: 0.1484 - precision_1: 0.9762 - recall_1: 0.9490 - weighted_f1_score: 0.7445 - val_f1_score: 0.8887 - val_loss: 0.1724 - val_precision_1: 0.9631 - val_recall_1: 0.9407 - val_weighted_f1_score: 0.8887\n",
      "Epoch 3/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 77ms/step - f1_score: 0.9098 - loss: 0.0818 - precision_1: 0.9879 - recall_1: 0.9735 - weighted_f1_score: 0.7688 - val_f1_score: 0.9010 - val_loss: 0.1924 - val_precision_1: 0.9593 - val_recall_1: 0.9512 - val_weighted_f1_score: 0.9010\n",
      "Epoch 4/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 76ms/step - f1_score: 0.9172 - loss: 0.0525 - precision_1: 0.9912 - recall_1: 0.9849 - weighted_f1_score: 0.7851 - val_f1_score: 0.9054 - val_loss: 0.2264 - val_precision_1: 0.9554 - val_recall_1: 0.9598 - val_weighted_f1_score: 0.9054\n",
      "Epoch 5/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 76ms/step - f1_score: 0.9203 - loss: 0.0415 - precision_1: 0.9923 - recall_1: 0.9897 - weighted_f1_score: 0.7919 - val_f1_score: 0.9116 - val_loss: 0.2344 - val_precision_1: 0.9518 - val_recall_1: 0.9660 - val_weighted_f1_score: 0.9116\n",
      "Epoch 6/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 76ms/step - f1_score: 0.9235 - loss: 0.0364 - precision_1: 0.9930 - recall_1: 0.9913 - weighted_f1_score: 0.7991 - val_f1_score: 0.9156 - val_loss: 0.2680 - val_precision_1: 0.9541 - val_recall_1: 0.9618 - val_weighted_f1_score: 0.9156\n",
      "Epoch 7/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 76ms/step - f1_score: 0.9255 - loss: 0.0342 - precision_1: 0.9934 - recall_1: 0.9920 - weighted_f1_score: 0.8037 - val_f1_score: 0.9191 - val_loss: 0.2711 - val_precision_1: 0.9550 - val_recall_1: 0.9606 - val_weighted_f1_score: 0.9191\n",
      "Epoch 8/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 76ms/step - f1_score: 0.9259 - loss: 0.0314 - precision_1: 0.9939 - recall_1: 0.9925 - weighted_f1_score: 0.8047 - val_f1_score: 0.9197 - val_loss: 0.2716 - val_precision_1: 0.9531 - val_recall_1: 0.9633 - val_weighted_f1_score: 0.9197\n",
      "Epoch 9/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 76ms/step - f1_score: 0.9282 - loss: 0.0297 - precision_1: 0.9942 - recall_1: 0.9929 - weighted_f1_score: 0.8099 - val_f1_score: 0.9180 - val_loss: 0.2831 - val_precision_1: 0.9520 - val_recall_1: 0.9648 - val_weighted_f1_score: 0.9180\n",
      "Epoch 10/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 76ms/step - f1_score: 0.9270 - loss: 0.0294 - precision_1: 0.9943 - recall_1: 0.9931 - weighted_f1_score: 0.8072 - val_f1_score: 0.9224 - val_loss: 0.2922 - val_precision_1: 0.9541 - val_recall_1: 0.9639 - val_weighted_f1_score: 0.9224\n"
     ]
    }
   ],
   "source": [
    "# Standard model\n",
    "# Define the model\n",
    "input_layer = Input(shape=(X_train.shape[1],), sparse=True)\n",
    "x1 = layers.Dense(1000, activation=\"relu\")(input_layer) # 1st hidden layer with 1000 neurons\n",
    "x1 = layers.Dropout(0.4)(x1)                            # Dropout layer to prevent overfitting \n",
    "x2 = layers.Dense(500, activation=\"relu\")(x1)           # 2nd hidden layer with 500 neurons\n",
    "x2 = layers.Dropout(0.2)(x2)                            # Dropout layer to prevent overfitting\n",
    "x3 = layers.Dense(100, activation=\"relu\")(x2)           # 3rd hidden layer with 100 neurons\n",
    "x4 = layers.Dense(25, activation=\"relu\")(x3)            # 4th hidden layer with 25 neurons\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x4)      # Output layer with 1 neuron and sigmoid to force binary\n",
    "\n",
    "# Create the model\n",
    "NN = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "#Initialize the NN\n",
    "NN.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.F1Score()],\n",
    "    weighted_metrics=[\"f1_score\"],  # Weight the F1 metric higher\n",
    ")\n",
    "\n",
    "# Define the callbacks for early stopping\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',     # Use F1 for early stopping\n",
    "        mode='max',                 # We want to \"maximize\" the F1 score  \n",
    "        patience=4,                 # If theres no improvements after 4 epochs, stop\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define the weights for the classes\n",
    "weight_for_0 = len(y_train) / (2.0 * (y_train == 0).sum())\n",
    "weight_for_1 = len(y_train) / (2.0 * (y_train == 1).sum())\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "#Train the NN\n",
    "history = NN.fit(\n",
    "    X_train,y_train,                    # Trainin data as input and expected output\n",
    "    validation_data = (X_val,y_val),    # Data for validation\n",
    "    epochs = 10,                        # Number of iterations over the entire dataset\n",
    "    batch_size = 1024,                  # Number of samples per gradient update\n",
    "    callbacks=callbacks,                # Use the callback\n",
    "    class_weight=class_weights          # Use the class weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "output_path = \"models/nn_classifier12_bigram.keras\"\n",
    "NN.save(output_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "input_path = \"models/nn_classifier12_bigram.keras\"\n",
    "NN = tf.keras.models.load_model(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_results = NN.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"* Test Loss: {test_results[0]:.4f}\")\n",
    "print(f\"* Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"* Test Precision: {test_results[2]:.4f}\")\n",
    "print(f\"* Test Recall: {test_results[3]:.4f}\")\n",
    "print(f\"* Test AUC: {test_results[4]:.4f}\")\n",
    "print(f\"* Test F1 Score: {test_results[5]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
